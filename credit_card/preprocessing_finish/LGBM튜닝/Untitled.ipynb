{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surrounded-calcium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'sample_submission.csv', 'test.csv', 'train.csv', 'Untitled.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "d = \"C:\\kaggle_data\\credit_card\"\n",
    "lst = os.listdir(d)\n",
    "print(lst)\n",
    "train = pd.read_csv(d + '\\\\' +lst[3])\n",
    "test = pd.read_csv(d + '\\\\' +lst[2])\n",
    "ss = pd.read_csv(d + '\\\\' +lst[1])\n",
    "train = train.drop(['index'], axis=1)\n",
    "train.fillna('NAN', inplace=True) \n",
    "test = test.drop(['index'], axis=1)\n",
    "test.fillna('NAN', inplace=True)\n",
    "\n",
    "# Married, Civil marriage\n",
    "train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= train['income_per_size'] * 2\n",
    "\n",
    "test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= test['income_per_size'] * 2\n",
    "\n",
    "def simple_marry(x):\n",
    "    if x == 'Married' or x =='Civil marriage':\n",
    "        return '0'\n",
    "    elif x == 'Separated' or x == 'Widow':\n",
    "        return '1'\n",
    "    else:\n",
    "        return '2'\n",
    "\n",
    "for df in [train,test]:\n",
    "    df['family_bins'] = df['family_type'].apply(simple_marry)\n",
    "\n",
    "# income_total을 로그변환\n",
    "train['income_total'] = np.log(train['income_total'])\n",
    "# train = train.drop('income_total',1)\n",
    "test['income_total'] = np.log(test['income_total'])\n",
    "# test = test.drop('income_total',1)\n",
    "\n",
    "# car와 reality를 합친 새로운 칼럼 careality\n",
    "train['car'] =train['car'].apply(lambda x: int(x=='Y'))\n",
    "train['reality'] =train['reality'].apply(lambda x: int(x=='Y'))\n",
    "test['car'] =test['car'].apply(lambda x: int(x=='Y'))\n",
    "test['reality'] =test['reality'].apply(lambda x: int(x=='Y'))\n",
    "\n",
    "train['careality'] = train['car'] + train['reality']\n",
    "train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "test['careality'] = test['car'] + test['reality']\n",
    "test = test.drop(['car', 'reality'],1)\n",
    "\n",
    "object_col = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train.drop(object_col, axis=1, inplace=True)\n",
    "train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test.drop(object_col, axis=1, inplace=True)\n",
    "test = pd.concat([test, test_onehot_df], axis=1)\n",
    "\n",
    "## 제곱\n",
    "for df in [train,test]:\n",
    "    df['income_per_size'] = df['income_per_size'].apply(lambda x: x**2)\n",
    "    \n",
    "c = 'income_per_size'\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'income_total'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'begin_month'\n",
    "k = 2.2\n",
    "\n",
    "train[c] = train[c].apply(lambda x: x**2)\n",
    "test[c] = test[c].apply(lambda x: x**2)\n",
    "\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "# train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "######################\n",
    "c = 'DAYS_BIRTH'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "#기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "# out: 아웃라이어\n",
    "out_train = train.loc[train.DAYS_EMPLOYED>0]\n",
    "out_test = test.loc[test.DAYS_EMPLOYED>0]\n",
    "\n",
    "in_train = train.loc[train.DAYS_EMPLOYED<=0]\n",
    "in_test = test.loc[test.DAYS_EMPLOYED<=0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "c = 'DAYS_EMPLOYED'\n",
    "k = 2.2\n",
    "mean = in_train[c].mean()\n",
    "std = in_train[c].std()\n",
    "idxs = in_train.loc[(in_train[c]>= mean + k*std)|\\\n",
    "                (in_train[c]<= mean - k*std)].index\n",
    "in_train = in_train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "in_train[c] = (in_train[c]-mean)/std\n",
    "in_test[c] = (in_test[c]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subjective-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "native-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgtrain = lightgbm.Dataset(out_train.drop('credit',1), out_train['credit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "numeric-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"objective\" : \"multiclass\",\n",
    "        \"metric\" : \"multi_logloss\", \n",
    "        \"num_classes\" : 3\n",
    "#         'is_unbalance': True,\n",
    "#         \"num_leaves\" : int(num_leaves),\n",
    "#         \"max_depth\" : int(max_depth),\n",
    "#         \"lambda_l2\" : lambda_l2,\n",
    "#         \"lambda_l1\" : lambda_l1,\n",
    "#         \"num_threads\" : 20,\n",
    "#         \"min_child_samples\" : int(min_child_samples),\n",
    "#         'min_data_in_leaf': int(min_data_in_leaf),\n",
    "#         \"learning_rate\" : 0.03,\n",
    "#         \"subsample_freq\" : 5,\n",
    "#         \"bagging_seed\" : 42,\n",
    "#         \"verbosity\" : -1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "visible-ivory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Find whitespaces in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 589\n",
      "[LightGBM] [Info] Number of data points in the train set: 2799, number of used features: 26\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 589\n",
      "[LightGBM] [Info] Number of data points in the train set: 2799, number of used features: 26\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 589\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.113972\n",
      "[LightGBM] [Info] Start training from score -1.410523\n",
      "[LightGBM] [Info] Start training from score -0.453773\n",
      "[LightGBM] [Info] Start training from score -2.113972\n",
      "[LightGBM] [Info] Start training from score -1.410523\n",
      "[LightGBM] [Info] Start training from score -0.453773\n",
      "[LightGBM] [Info] Start training from score -2.114329\n",
      "[LightGBM] [Info] Start training from score -1.409417\n",
      "[LightGBM] [Info] Start training from score -0.454130\n"
     ]
    }
   ],
   "source": [
    "cv_result = lightgbm.cv(params,\n",
    "                       lgtrain,\n",
    "                       1000,\n",
    "                       early_stopping_rounds=100,\n",
    "                       stratified=True,\n",
    "                       nfold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ignored-field",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7554990214293098"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-cv_result['multi_logloss-mean'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "return cv_result['auc-mean'][-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
