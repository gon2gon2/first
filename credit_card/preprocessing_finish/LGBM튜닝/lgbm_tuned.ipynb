{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/clair14/lgbm-bayesianoptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'sample_submission.csv', 'test.csv', 'train.csv', 'Untitled.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import log_loss\n",
    "from bayes_opt import BayesianOptimization\n",
    "import lightgbm\n",
    "\n",
    "\n",
    "\n",
    "d = \"C:\\kaggle_data\\credit_card\"\n",
    "lst = os.listdir(d)\n",
    "print(lst)\n",
    "train = pd.read_csv(d + '\\\\' +lst[3])\n",
    "test = pd.read_csv(d + '\\\\' +lst[2])\n",
    "ss = pd.read_csv(d + '\\\\' +lst[1])\n",
    "train = train.drop(['index'], axis=1)\n",
    "train.fillna('NAN', inplace=True) \n",
    "test = test.drop(['index'], axis=1)\n",
    "test.fillna('NAN', inplace=True)\n",
    "\n",
    "\n",
    "# Married, Civil marriage\n",
    "train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= train['income_per_size'] * 2\n",
    "\n",
    "test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= test['income_per_size'] * 2\n",
    "\n",
    "def simple_marry(x):\n",
    "    if x == 'Married' or x =='Civil marriage':\n",
    "        return '0'\n",
    "    elif x == 'Separated' or x == 'Widow':\n",
    "        return '1'\n",
    "    else:\n",
    "        return '2'\n",
    "\n",
    "for df in [train,test]:\n",
    "    df['family_bins'] = df['family_type'].apply(simple_marry)\n",
    "\n",
    "# income_total을 로그변환\n",
    "train['income_total'] = np.log(train['income_total'])\n",
    "# train = train.drop('income_total',1)\n",
    "test['income_total'] = np.log(test['income_total'])\n",
    "# test = test.drop('income_total',1)\n",
    "\n",
    "# car와 reality를 합친 새로운 칼럼 careality\n",
    "train['car'] =train['car'].apply(lambda x: int(x=='Y'))\n",
    "train['reality'] =train['reality'].apply(lambda x: int(x=='Y'))\n",
    "test['car'] =test['car'].apply(lambda x: int(x=='Y'))\n",
    "test['reality'] =test['reality'].apply(lambda x: int(x=='Y'))\n",
    "\n",
    "train['careality'] = train['car'] + train['reality']\n",
    "train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "test['careality'] = test['car'] + test['reality']\n",
    "test = test.drop(['car', 'reality'],1)\n",
    "\n",
    "train.drop('gender',1,inplace=True)\n",
    "test.drop('gender',1,inplace=True)\n",
    "\n",
    "object_col = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train.drop(object_col, axis=1, inplace=True)\n",
    "train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test.drop(object_col, axis=1, inplace=True)\n",
    "test = pd.concat([test, test_onehot_df], axis=1)\n",
    "\n",
    "\n",
    "## 제곱\n",
    "for df in [train,test]:\n",
    "    df['income_per_size'] = df['income_per_size'].apply(lambda x: x**2)\n",
    "    \n",
    "c = 'income_per_size'\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'income_total'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'begin_month'\n",
    "k = 2.2\n",
    "\n",
    "train[c] = train[c].apply(lambda x: x**2)\n",
    "test[c] = test[c].apply(lambda x: x**2)\n",
    "\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "# train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "######################\n",
    "c = 'DAYS_BIRTH'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "#기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "c = 'DAYS_EMPLOYED'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "ran = ['child_num', 'DAYS_BIRTH', 'DAYS_EMPLOYED','family_size', 'begin_month', 'income_per_size']\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "tf = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution='normal')\n",
    "tf.fit(train[ran])\n",
    "train[ran] = tf.transform(train[ran])\n",
    "test[ran] = tf.transform(test[ran])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 튜닝 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$$$$$$$$$$$$$$$$$$ 1 번째 $$$$$$$$$$$$$$$$$$$$$$$\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 19358, number of used features: 52\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score -2.108690\n",
      "[LightGBM] [Info] Start training from score -1.441188\n",
      "[LightGBM] [Info] Start training from score -0.443234\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.746878\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's multi_logloss: 0.727274\n",
      "[300]\tvalid_0's multi_logloss: 0.721628\n",
      "Early stopping, best iteration is:\n",
      "[367]\tvalid_0's multi_logloss: 0.720029\n",
      "$$$$$$$$$$$$$$$$$$$$$$$ 2 번째 $$$$$$$$$$$$$$$$$$$$$$$\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 1026\n",
      "[LightGBM] [Info] Number of data points in the train set: 19358, number of used features: 52\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score -2.108690\n",
      "[LightGBM] [Info] Start training from score -1.440970\n",
      "[LightGBM] [Info] Start training from score -0.443315\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's multi_logloss: 0.737318\n",
      "[200]\tvalid_0's multi_logloss: 0.722636\n",
      "[300]\tvalid_0's multi_logloss: 0.71461\n",
      "Early stopping, best iteration is:\n",
      "[285]\tvalid_0's multi_logloss: 0.713877\n",
      "$$$$$$$$$$$$$$$$$$$$$$$ 3 번째 $$$$$$$$$$$$$$$$$$$$$$$\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 1024\n",
      "[LightGBM] [Info] Number of data points in the train set: 19358, number of used features: 52\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score -2.108690\n",
      "[LightGBM] [Info] Start training from score -1.440970\n",
      "[LightGBM] [Info] Start training from score -0.443315\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's multi_logloss: 0.751321\n",
      "[200]\tvalid_0's multi_logloss: 0.735245\n",
      "[300]\tvalid_0's multi_logloss: 0.726596\n",
      "Early stopping, best iteration is:\n",
      "[310]\tvalid_0's multi_logloss: 0.72573\n",
      "$$$$$$$$$$$$$$$$$$$$$$$ 4 번째 $$$$$$$$$$$$$$$$$$$$$$$\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 1023\n",
      "[LightGBM] [Info] Number of data points in the train set: 19358, number of used features: 52\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score -2.108690\n",
      "[LightGBM] [Info] Start training from score -1.440970\n",
      "[LightGBM] [Info] Start training from score -0.443315\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.736665\n",
      "[200]\tvalid_0's multi_logloss: 0.712475\n",
      "[300]\tvalid_0's multi_logloss: 0.704315\n",
      "Early stopping, best iteration is:\n",
      "[344]\tvalid_0's multi_logloss: 0.702164\n",
      "$$$$$$$$$$$$$$$$$$$$$$$ 5 번째 $$$$$$$$$$$$$$$$$$$$$$$\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 1029\n",
      "[LightGBM] [Info] Number of data points in the train set: 19358, number of used features: 52\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score -2.108690\n",
      "[LightGBM] [Info] Start training from score -1.440970\n",
      "[LightGBM] [Info] Start training from score -0.443315\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's multi_logloss: 0.738424\n",
      "[200]\tvalid_0's multi_logloss: 0.714252\n",
      "[300]\tvalid_0's multi_logloss: 0.707472\n",
      "Early stopping, best iteration is:\n",
      "[367]\tvalid_0's multi_logloss: 0.704899\n",
      "$$$$$$$$$$$$$$$$$$$$$$$ 6 번째 $$$$$$$$$$$$$$$$$$$$$$$\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 1025\n",
      "[LightGBM] [Info] Number of data points in the train set: 19358, number of used features: 52\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score -2.108690\n",
      "[LightGBM] [Info] Start training from score -1.440970\n",
      "[LightGBM] [Info] Start training from score -0.443315\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.742612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's multi_logloss: 0.730928\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's multi_logloss: 0.727203\n",
      "$$$$$$$$$$$$$$$$$$$$$$$ 7 번째 $$$$$$$$$$$$$$$$$$$$$$$\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 1026\n",
      "[LightGBM] [Info] Number of data points in the train set: 19358, number of used features: 51\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score -2.108690\n",
      "[LightGBM] [Info] Start training from score -1.440970\n",
      "[LightGBM] [Info] Start training from score -0.443315\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's multi_logloss: 0.742684\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's multi_logloss: 0.72135\n",
      "[300]\tvalid_0's multi_logloss: 0.714593\n",
      "[400]\tvalid_0's multi_logloss: 0.70968\n",
      "Early stopping, best iteration is:\n",
      "[387]\tvalid_0's multi_logloss: 0.708717\n",
      "$$$$$$$$$$$$$$$$$$$$$$$ 8 번째 $$$$$$$$$$$$$$$$$$$$$$$\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 1023\n",
      "[LightGBM] [Info] Number of data points in the train set: 19358, number of used features: 51\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score -2.108690\n",
      "[LightGBM] [Info] Start training from score -1.440970\n",
      "[LightGBM] [Info] Start training from score -0.443315\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's multi_logloss: 0.747703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's multi_logloss: 0.726631\n",
      "[300]\tvalid_0's multi_logloss: 0.718503\n",
      "Early stopping, best iteration is:\n",
      "[364]\tvalid_0's multi_logloss: 0.714468\n",
      "$$$$$$$$$$$$$$$$$$$$$$$ 9 번째 $$$$$$$$$$$$$$$$$$$$$$$\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 1022\n",
      "[LightGBM] [Info] Number of data points in the train set: 19358, number of used features: 51\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score -2.109116\n",
      "[LightGBM] [Info] Start training from score -1.440970\n",
      "[LightGBM] [Info] Start training from score -0.443234\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's multi_logloss: 0.741486\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's multi_logloss: 0.717642\n",
      "[300]\tvalid_0's multi_logloss: 0.710669\n",
      "Early stopping, best iteration is:\n",
      "[345]\tvalid_0's multi_logloss: 0.708694\n",
      "$$$$$$$$$$$$$$$$$$$$$$$ 10 번째 $$$$$$$$$$$$$$$$$$$$$$$\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Total Bins 1023\n",
      "[LightGBM] [Info] Number of data points in the train set: 19359, number of used features: 51\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score -2.108742\n",
      "[LightGBM] [Info] Start training from score -1.441022\n",
      "[LightGBM] [Info] Start training from score -0.443286\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.743944\n",
      "[200]\tvalid_0's multi_logloss: 0.723042\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\tvalid_0's multi_logloss: 0.718938\n",
      "Early stopping, best iteration is:\n",
      "[277]\tvalid_0's multi_logloss: 0.718459\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "loss = 0\n",
    "skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "folds=[]\n",
    "oof = []\n",
    "for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "random.seed(42)\n",
    "models={}\n",
    "params = {\n",
    "        \"boosting\": 'gbdt',\n",
    "        \"objective\" : \"multiclass\",\n",
    "        \"metric\" : \"multi_logloss\", \n",
    "        \"num_classes\" : 3,\n",
    "        \"n_estimators\": 1000,  \n",
    "            \"max_depth\": 9,            \n",
    "        \"bagging_fraction\": 0.9, \n",
    "        \"bagging_freq\":2,         \n",
    "        \"feature_fraction\": 0.7, \n",
    "        'force_col_wise':True,\n",
    "        \"nthreads\":44,\n",
    "        \"seed\":42}\n",
    "\n",
    "\n",
    "for fold in range(n):\n",
    "    print('$$$$$$$$$$$$$$$$$$$$$$$',fold+1,'번째','$$$$$$$$$$$$$$$$$$$$$$$')\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                         train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "    lgtrain = lightgbm.Dataset(X_train, y_train)\n",
    "    lgeval = lightgbm.Dataset(X_valid,y_valid)\n",
    "    model = lightgbm.train(params=params,train_set=lgtrain,\n",
    "                           valid_sets=lgeval,\n",
    "                           verbose_eval=100,\n",
    "                          early_stopping_rounds=30)\n",
    "    \n",
    "    y_pred = model.predict(X_valid)\n",
    "    oof.append(y_pred)\n",
    "    loss += log_loss(y_valid,y_pred)\n",
    "    models[fold] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.1336151 , 0.25567454, 0.61071036],\n",
       "        [0.04156198, 0.13241787, 0.82602015],\n",
       "        [0.07196919, 0.16882673, 0.75920408],\n",
       "        ...,\n",
       "        [0.05752408, 0.29496355, 0.64751237],\n",
       "        [0.04453864, 0.09159492, 0.86386644],\n",
       "        [0.12875602, 0.1372455 , 0.73399848]]),\n",
       " array([[3.34480478e-01, 5.64662363e-02, 6.09053285e-01],\n",
       "        [5.03428749e-01, 4.95692220e-01, 8.79031489e-04],\n",
       "        [4.05472346e-01, 5.93084557e-01, 1.44309792e-03],\n",
       "        ...,\n",
       "        [7.25854612e-02, 1.32160086e-01, 7.95254453e-01],\n",
       "        [1.50836724e-01, 8.45640605e-01, 3.52267087e-03],\n",
       "        [2.69407960e-02, 8.81272491e-02, 8.84931955e-01]]),\n",
       " array([[0.09785006, 0.13297263, 0.76917731],\n",
       "        [0.08384002, 0.1280291 , 0.78813089],\n",
       "        [0.03982453, 0.62237681, 0.33779866],\n",
       "        ...,\n",
       "        [0.13494813, 0.13068058, 0.73437129],\n",
       "        [0.0882203 , 0.15050065, 0.76127905],\n",
       "        [0.04321474, 0.07700779, 0.87977748]]),\n",
       " array([[0.06206787, 0.24801157, 0.68992056],\n",
       "        [0.04720883, 0.16157576, 0.79121541],\n",
       "        [0.05151967, 0.10068519, 0.84779514],\n",
       "        ...,\n",
       "        [0.22504061, 0.10720392, 0.66775547],\n",
       "        [0.19538482, 0.0627147 , 0.74190048],\n",
       "        [0.11542285, 0.07144164, 0.81313551]]),\n",
       " array([[0.16371971, 0.10331968, 0.73296062],\n",
       "        [0.41598579, 0.08684305, 0.49717116],\n",
       "        [0.19017481, 0.80859462, 0.00123057],\n",
       "        ...,\n",
       "        [0.40061613, 0.17139909, 0.42798478],\n",
       "        [0.08304993, 0.11564558, 0.80130448],\n",
       "        [0.04781407, 0.29542718, 0.65675874]]),\n",
       " array([[0.54497016, 0.10607249, 0.34895736],\n",
       "        [0.05812916, 0.14593252, 0.79593832],\n",
       "        [0.1324759 , 0.21418824, 0.65333587],\n",
       "        ...,\n",
       "        [0.06422551, 0.13384784, 0.80192665],\n",
       "        [0.04865372, 0.41737678, 0.5339695 ],\n",
       "        [0.171967  , 0.59175847, 0.23627453]]),\n",
       " array([[9.95489106e-02, 6.50859979e-02, 8.35365092e-01],\n",
       "        [7.54266895e-02, 2.29567243e-01, 6.95006068e-01],\n",
       "        [1.61943992e-01, 1.29145277e-01, 7.08910731e-01],\n",
       "        ...,\n",
       "        [3.90468700e-02, 5.26955020e-01, 4.33998110e-01],\n",
       "        [6.00180971e-02, 9.39205216e-01, 7.76687125e-04],\n",
       "        [9.86061109e-02, 4.78087822e-01, 4.23306067e-01]]),\n",
       " array([[0.00999019, 0.04170436, 0.94830545],\n",
       "        [0.25811136, 0.74060622, 0.00128243],\n",
       "        [0.1901362 , 0.12871278, 0.68115103],\n",
       "        ...,\n",
       "        [0.01536549, 0.23289497, 0.75173954],\n",
       "        [0.05094204, 0.52890774, 0.42015023],\n",
       "        [0.12995357, 0.45322868, 0.41681775]]),\n",
       " array([[0.08233004, 0.04863013, 0.86903984],\n",
       "        [0.08163244, 0.17027363, 0.74809393],\n",
       "        [0.06608472, 0.17270696, 0.76120832],\n",
       "        ...,\n",
       "        [0.49514041, 0.09247049, 0.4123891 ],\n",
       "        [0.10010186, 0.44159642, 0.45830172],\n",
       "        [0.13567538, 0.43994803, 0.42437658]]),\n",
       " array([[0.099526  , 0.18241943, 0.71805458],\n",
       "        [0.13557428, 0.1285422 , 0.73588352],\n",
       "        [0.16659082, 0.2183492 , 0.61505998],\n",
       "        ...,\n",
       "        [0.13818764, 0.24429272, 0.61751964],\n",
       "        [0.03450358, 0.49482957, 0.47066685],\n",
       "        [0.03309323, 0.09275432, 0.87415246]])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.144239538357529"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min_child_samples, min_child_weight 활성화\n",
    "# loss = 7.1728937306252885\n",
    "\n",
    "# min_child_samples, min_child_weight 비활성화\n",
    "# loss = 7.144239538357529\n",
    "\n",
    "# force_col_wise까지 비활성화\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.iloc[:,1:]=0\n",
    "for fold in range(n):\n",
    "    ss.iloc[:,1:] += models[fold].predict(test)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('lgbm_tuned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('[0]\\W[1-9]{4,5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8491\n",
      "0.82522\n"
     ]
    }
   ],
   "source": [
    "for i in p.finditer(a):\n",
    "    print(i.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.80141\n",
      "Best weights from best epoch are automatically used!\n",
      "5 3 3\n",
      "\n",
      "Early stopping occurred at epoch 214 with best_epoch \n",
      " 0.80541\n",
      "Best weights from best epoch are automatically used!\n",
      " 50%|██████████████████                  | 5/10 [06:34<06:28, 77.67s/it]\n",
      "5 3 3\n",
      " 60%|█████████████████████▌              | 6/10 [07:45<05:02, 75.51s/it]\n",
      "\n",
      "Early stopping occurred at epoch 182 with best_epoch \n",
      " 0.8057\n",
      "Best weights from best epoch are automatically used!\n",
      "5 3 3\n",
      " 70%|█████████████████████████▏          | 7/10 [08:23<03:09, 63.15s/it]\n",
      "\n",
      "Early stopping occurred at epoch 95 with best_epoch \n",
      " 0.8491\n",
      "Best weights from best epoch are automatically used!\n",
      "5 3 3\n",
      "\n",
      "Early stopping occurred at epoch 167 with best_epoch \n",
      " 0.82522\n",
      "Best weights from best epoch are automatically used!\n",
      " 80%|████████████████████████████▊       | 8/10 [09:28<02:07, 63.93s/it]\n",
      "5 3 3\n",
      " 90%|████████████████████████████████▍   | 9/10 [10:28<01:02, 62.54s/it]\n",
      "\n",
      "Early stopping occurred at epoch 151 with best_epoch \n",
      " 0.8208\n",
      "Best weights from best epoch are automatically used!\n",
      "5 3 3\n",
      "100%|███████████████████████████████████| 10/10 [11:45<00:00, 70.51s/it]\n",
      "  0%|                                            | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "Early stopping occurred at epoch 196 with best_epoch \n",
      " 0.80972\n"
     ]
    }
   ],
   "source": [
    "for z in a.split('='):\n",
    "    if z.startswith(' 0.'):\n",
    "        print(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
