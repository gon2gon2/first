{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.kaggle.com/clair14/lgbm-bayesianoptimization","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport glob\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder\nimport random\nimport os\nfrom sklearn.metrics import log_loss\nfrom bayes_opt import BayesianOptimization\nimport lightgbm\n\ntrain = pd.read_csv('../input/cc123123123/train.csv')\ntest = pd.read_csv('../input/cc123123123/test.csv')\nss = pd.read_csv('../input/cc123123123/sample_submission.csv')\ntrain = train.drop(['index'], axis=1)\ntrain.fillna('NAN', inplace=True) \ntest = test.drop(['index'], axis=1)\ntest.fillna('NAN', inplace=True)\n\n# Married, Civil marriage\ntrain['income_per_size'] = np.log(train['income_total']/train['family_size'])\ntest['income_per_size'] = np.log(test['income_total']/test['family_size'])\ntrain.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n= train['income_per_size'] * 2\n\ntest.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n= test['income_per_size'] * 2\n\ndef simple_marry(x):\n    if x == 'Married' or x =='Civil marriage':\n        return '0'\n    elif x == 'Separated' or x == 'Widow':\n        return '1'\n    else:\n        return '2'\n\nfor df in [train,test]:\n    df['family_bins'] = df['family_type'].apply(simple_marry)\n\n# income_total을 로그변환\ntrain['income_total'] = np.log(train['income_total'])\n# train = train.drop('income_total',1)\ntest['income_total'] = np.log(test['income_total'])\n# test = test.drop('income_total',1)\n\n# car와 reality를 합친 새로운 칼럼 careality\ntrain['car'] =train['car'].apply(lambda x: int(x=='Y'))\ntrain['reality'] =train['reality'].apply(lambda x: int(x=='Y'))\ntest['car'] =test['car'].apply(lambda x: int(x=='Y'))\ntest['reality'] =test['reality'].apply(lambda x: int(x=='Y'))\n\ntrain['careality'] = train['car'] + train['reality']\ntrain = train.drop(['car', 'reality'],1)\n\ntest['careality'] = test['car'] + test['reality']\ntest = test.drop(['car', 'reality'],1)\n\nobject_col = []\nfor col in train.columns:\n    if train[col].dtype == 'object':\n        object_col.append(col)\n\nenc = OneHotEncoder()\nenc.fit(train.loc[:,object_col])\n\n\ntrain_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n             columns=enc.get_feature_names(object_col))\ntrain.drop(object_col, axis=1, inplace=True)\ntrain = pd.concat([train, train_onehot_df], axis=1)\n\ntest_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n             columns=enc.get_feature_names(object_col))\ntest.drop(object_col, axis=1, inplace=True)\ntest = pd.concat([test, test_onehot_df], axis=1)\n\n## 제곱\nfor df in [train,test]:\n    df['income_per_size'] = df['income_per_size'].apply(lambda x: x**2)\n    \nc = 'income_per_size'\nmean = train[c].mean()\nstd = train[c].std()\n\n# 기존 \ntrain[c] = (train[c]-mean)/std\ntest[c] = (test[c]-mean)/std\n\n#####################\nc = 'income_total'\nk = 2.2\nmean = train[c].mean()\nstd = train[c].std()\nidxs = train.loc[(train[c]>= mean + k*std)|\\\n                (train[c]<= mean - k*std)].index\ntrain = train.drop(idxs).reset_index(drop=True)\n\n# 기존 \ntrain[c] = (train[c]-mean)/std\ntest[c] = (test[c]-mean)/std\n\n#####################\nc = 'begin_month'\nk = 2.2\n\ntrain[c] = train[c].apply(lambda x: x**2)\ntest[c] = test[c].apply(lambda x: x**2)\n\nmean = train[c].mean()\nstd = train[c].std()\n\nidxs = train.loc[(train[c]>= mean + k*std)|\\\n                (train[c]<= mean - k*std)].index\n# train = train.drop(idxs).reset_index(drop=True)\n\n# 기존 \ntrain[c] = (train[c]-mean)/std\ntest[c] = (test[c]-mean)/std\n\n######################\nc = 'DAYS_BIRTH'\nk = 2.2\nmean = train[c].mean()\nstd = train[c].std()\nidxs = train.loc[(train[c]>= mean + k*std)|\\\n                (train[c]<= mean - k*std)].index\ntrain = train.drop(idxs).reset_index(drop=True)\n\n#기존 \ntrain[c] = (train[c]-mean)/std\ntest[c] = (test[c]-mean)/std\n\n\n\n##########################\n# out: 아웃라이어\nout_train = train.loc[train.DAYS_EMPLOYED>0]\nout_test = test.loc[test.DAYS_EMPLOYED>0]\n\nin_train = train.loc[train.DAYS_EMPLOYED<=0]\nin_test = test.loc[test.DAYS_EMPLOYED<=0]\n\n\n\n\n###################\nc = 'DAYS_EMPLOYED'\nk = 2.2\nmean = in_train[c].mean()\nstd = in_train[c].std()\nidxs = in_train.loc[(in_train[c]>= mean + k*std)|\\\n                (in_train[c]<= mean - k*std)].index\nin_train = in_train.drop(idxs).reset_index(drop=True)\n\n# 기존 \nin_train[c] = (in_train[c]-mean)/std\nin_test[c] = (in_test[c]-mean)/std","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# #######모델링, 학습 시작#######\n\n|365243?|학습|예측|\n|----|----|----|\n|yes|out_train|out_test|\n|no|in_train|in_test|","metadata":{}},{"cell_type":"code","source":"# !pip install bayesian-optimization","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"temp = in_train\ndef lgb_eval(n_estimators, num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf, learning_rate):\n    \n\n    params = {\n        \"objective\" : \"multiclass\",\n        \"metric\" : \"multi_logloss\", \n        \"num_classes\" : 3,\n        'is_unbalance': True,\n        'n_estimators': int(n_estimators),\n        \"num_leaves\" : int(num_leaves),\n        \"max_depth\" : int(max_depth),\n        \"lambda_l2\" : lambda_l2,\n        \"lambda_l1\" : lambda_l1,\n        \"num_threads\" : 20,\n        \"min_child_samples\" : int(min_child_samples),\n        'min_data_in_leaf': int(min_data_in_leaf),\n        \"learning_rate\" : learning_rate,\n        \"subsample_freq\" : 5,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1\n    }\n    lgtrain = lightgbm.Dataset(temp.drop('credit',1), temp['credit'])\n    cv_result = lightgbm.cv(params,\n                       lgtrain,\n                       1000,\n                       early_stopping_rounds=100,\n                       stratified=True,\n                       nfold=10)\n    return -cv_result['multi_logloss-mean'][-1]","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n                                        'n_estimators': (300,2000),\n                                                'max_depth': (5, 63),\n                                                'lambda_l2': (0.0, 0.05),\n                                                'lambda_l1': (0.0, 0.05),\n                                                'min_child_samples': (50, 10000),\n                                                'min_data_in_leaf': (100, 2000),\n                                                'learning_rate': (0.01, 0.3)\n                                                })\n\nlgbBO.maximize(n_iter=30, init_points=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"|   iter    |  target   | lambda_l1 | lambda_l2 | learni... | max_depth | min_ch... | min_da... | n_esti... | num_le... |\n-------------------------------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"lgbBO.max","metadata":{"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'target': -0.7666262326076231,\n 'params': {'lambda_l1': 0.04619401127783732,\n  'lambda_l2': 0.023516340914865916,\n  'learning_rate': 0.21142050117535985,\n  'max_depth': 9.817818031569452,\n  'min_child_samples': 1069.5789469296615,\n  'min_data_in_leaf': 159.20038594955219,\n  'n_estimators': 1440.4418354272257,\n  'num_leaves': 3976.0290566566396}}"},"metadata":{}}]},{"cell_type":"markdown","source":"# #######모델링, 학습 끝#######","metadata":{}},{"cell_type":"code","source":"in_zeros = np.zeros([len(in_test),3])\nfor fold in range(n):\n    in_zeros += in_models[fold].predict_proba(in_test)/n\nin_output = pd.DataFrame(in_zeros)\nin_output = in_output.reindex(index=pd.Index(idx_in_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_zeros = np.zeros([len(out_test),3])\nfor fold in range(n):\n    out_zeros += out_models[fold].predict_proba(out_test)/n\nout_output = pd.DataFrame(out_zeros)\nout_output = out_output.reindex(index=pd.Index(idx_out_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"in2 = np.concatenate((in_zeros,np.array(idx_in_test).reshape(-1,1)),axis=1)\nout2 = np.concatenate((out_zeros,np.array(idx_out_test).reshape(-1,1)),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame(np.concatenate((in2,out2),axis=0),columns=[0,1,2,'index'])\noutput['index'] = output['index'].astype('int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(output)):\n    row = output.loc[output['index']==i]\n    ss.iloc[i,1:] = row.iloc[:,:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss.to_csv('two_models.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}