{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "appointed-acrobat",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/clair14/lgbm-bayesianoptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "quantitative-shore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'sample_submission.csv', 'test.csv', 'train.csv', 'Untitled.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "d = \"C:\\kaggle_data\\credit_card\"\n",
    "lst = os.listdir(d)\n",
    "print(lst)\n",
    "train = pd.read_csv(d + '\\\\' +lst[3])\n",
    "test = pd.read_csv(d + '\\\\' +lst[2])\n",
    "ss = pd.read_csv(d + '\\\\' +lst[1])\n",
    "train = train.drop(['index'], axis=1)\n",
    "train.fillna('NAN', inplace=True) \n",
    "test = test.drop(['index'], axis=1)\n",
    "test.fillna('NAN', inplace=True)\n",
    "\n",
    "# Married, Civil marriage\n",
    "train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= train['income_per_size'] * 2\n",
    "\n",
    "test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= test['income_per_size'] * 2\n",
    "\n",
    "def simple_marry(x):\n",
    "    if x == 'Married' or x =='Civil marriage':\n",
    "        return '0'\n",
    "    elif x == 'Separated' or x == 'Widow':\n",
    "        return '1'\n",
    "    else:\n",
    "        return '2'\n",
    "\n",
    "for df in [train,test]:\n",
    "    df['family_bins'] = df['family_type'].apply(simple_marry)\n",
    "\n",
    "# income_total을 로그변환\n",
    "train['income_total'] = np.log(train['income_total'])\n",
    "# train = train.drop('income_total',1)\n",
    "test['income_total'] = np.log(test['income_total'])\n",
    "# test = test.drop('income_total',1)\n",
    "\n",
    "# car와 reality를 합친 새로운 칼럼 careality\n",
    "train['car'] =train['car'].apply(lambda x: int(x=='Y'))\n",
    "train['reality'] =train['reality'].apply(lambda x: int(x=='Y'))\n",
    "test['car'] =test['car'].apply(lambda x: int(x=='Y'))\n",
    "test['reality'] =test['reality'].apply(lambda x: int(x=='Y'))\n",
    "\n",
    "train['careality'] = train['car'] + train['reality']\n",
    "train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "test['careality'] = test['car'] + test['reality']\n",
    "test = test.drop(['car', 'reality'],1)\n",
    "\n",
    "object_col = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train.drop(object_col, axis=1, inplace=True)\n",
    "train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test.drop(object_col, axis=1, inplace=True)\n",
    "test = pd.concat([test, test_onehot_df], axis=1)\n",
    "\n",
    "## 제곱\n",
    "for df in [train,test]:\n",
    "    df['income_per_size'] = df['income_per_size'].apply(lambda x: x**2)\n",
    "    \n",
    "c = 'income_per_size'\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'income_total'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'begin_month'\n",
    "k = 2.2\n",
    "\n",
    "train[c] = train[c].apply(lambda x: x**2)\n",
    "test[c] = test[c].apply(lambda x: x**2)\n",
    "\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "# train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "######################\n",
    "c = 'DAYS_BIRTH'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "#기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "# out: 아웃라이어\n",
    "out_train = train.loc[train.DAYS_EMPLOYED>0]\n",
    "out_test = test.loc[test.DAYS_EMPLOYED>0]\n",
    "\n",
    "in_train = train.loc[train.DAYS_EMPLOYED<=0]\n",
    "in_test = test.loc[test.DAYS_EMPLOYED<=0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "c = 'DAYS_EMPLOYED'\n",
    "k = 2.2\n",
    "mean = in_train[c].mean()\n",
    "std = in_train[c].std()\n",
    "idxs = in_train.loc[(in_train[c]>= mean + k*std)|\\\n",
    "                (in_train[c]<= mean - k*std)].index\n",
    "in_train = in_train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "in_train[c] = (in_train[c]-mean)/std\n",
    "in_test[c] = (in_test[c]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "corresponding-maria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25708"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "smooth-gateway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20535"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(in_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "clean-crystal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4199"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-dating",
   "metadata": {},
   "source": [
    "# #######모델링, 학습 시작#######\n",
    "\n",
    "|365243?|학습|예측|\n",
    "|----|----|----|\n",
    "|yes|out_train|out_test|\n",
    "|no|in_train|in_test|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "primary-waters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
      "Building wheels for collected packages: bayesian-optimization\n",
      "  Building wheel for bayesian-optimization (setup.py): started\n",
      "  Building wheel for bayesian-optimization (setup.py): finished with status 'done'\n",
      "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=12969a168a51b80a0b429a37d0a5ee1fce10efd5534eea80716ecb7b2afe6494\n",
      "  Stored in directory: c:\\users\\fizz5\\appdata\\local\\pip\\cache\\wheels\\fd\\9b\\71\\f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
      "Successfully built bayesian-optimization\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "permanent-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nutritional-lodge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "child_num                                   int64\n",
       "income_total                              float64\n",
       "DAYS_BIRTH                                float64\n",
       "DAYS_EMPLOYED                             float64\n",
       "FLAG_MOBIL                                  int64\n",
       "work_phone                                  int64\n",
       "phone                                       int64\n",
       "email                                       int64\n",
       "family_size                               float64\n",
       "begin_month                               float64\n",
       "credit                                    float64\n",
       "income_per_size                           float64\n",
       "careality                                   int64\n",
       "gender_F                                  float64\n",
       "gender_M                                  float64\n",
       "income_type_Commercial associate          float64\n",
       "income_type_Pensioner                     float64\n",
       "income_type_State servant                 float64\n",
       "income_type_Student                       float64\n",
       "income_type_Working                       float64\n",
       "edu_type_Academic degree                  float64\n",
       "edu_type_Higher education                 float64\n",
       "edu_type_Incomplete higher                float64\n",
       "edu_type_Lower secondary                  float64\n",
       "edu_type_Secondary / secondary special    float64\n",
       "family_type_Civil marriage                float64\n",
       "family_type_Married                       float64\n",
       "family_type_Separated                     float64\n",
       "family_type_Single / not married          float64\n",
       "family_type_Widow                         float64\n",
       "house_type_Co-op apartment                float64\n",
       "house_type_House / apartment              float64\n",
       "house_type_Municipal apartment            float64\n",
       "house_type_Office apartment               float64\n",
       "house_type_Rented apartment               float64\n",
       "house_type_With parents                   float64\n",
       "occyp_type_Accountants                    float64\n",
       "occyp_type_Cleaning staff                 float64\n",
       "occyp_type_Cooking staff                  float64\n",
       "occyp_type_Core staff                     float64\n",
       "occyp_type_Drivers                        float64\n",
       "occyp_type_HR staff                       float64\n",
       "occyp_type_High skill tech staff          float64\n",
       "occyp_type_IT staff                       float64\n",
       "occyp_type_Laborers                       float64\n",
       "occyp_type_Low-skill Laborers             float64\n",
       "occyp_type_Managers                       float64\n",
       "occyp_type_Medicine staff                 float64\n",
       "occyp_type_NAN                            float64\n",
       "occyp_type_Private service staff          float64\n",
       "occyp_type_Realty agents                  float64\n",
       "occyp_type_Sales staff                    float64\n",
       "occyp_type_Secretaries                    float64\n",
       "occyp_type_Security staff                 float64\n",
       "occyp_type_Waiters/barmen staff           float64\n",
       "family_bins_0                             float64\n",
       "family_bins_1                             float64\n",
       "family_bins_2                             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "handled-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "biblical-family",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>child_num</th>\n",
       "      <th>income_total</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>work_phone</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>family_size</th>\n",
       "      <th>begin_month</th>\n",
       "      <th>credit</th>\n",
       "      <th>...</th>\n",
       "      <th>occyp_type_NAN</th>\n",
       "      <th>occyp_type_Private service staff</th>\n",
       "      <th>occyp_type_Realty agents</th>\n",
       "      <th>occyp_type_Sales staff</th>\n",
       "      <th>occyp_type_Secretaries</th>\n",
       "      <th>occyp_type_Security staff</th>\n",
       "      <th>occyp_type_Waiters/barmen staff</th>\n",
       "      <th>family_bins_0</th>\n",
       "      <th>family_bins_1</th>\n",
       "      <th>family_bins_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [child_num, income_total, DAYS_BIRTH, FLAG_MOBIL, work_phone, phone, email, family_size, begin_month, credit, income_per_size, careality, gender_F, gender_M, income_type_Commercial associate, income_type_Pensioner, income_type_State servant, income_type_Student, income_type_Working, edu_type_Academic degree, edu_type_Higher education, edu_type_Incomplete higher, edu_type_Lower secondary, edu_type_Secondary / secondary special, family_type_Civil marriage, family_type_Married, family_type_Separated, family_type_Single / not married, family_type_Widow, house_type_Co-op apartment, house_type_House / apartment, house_type_Municipal apartment, house_type_Office apartment, house_type_Rented apartment, house_type_With parents, occyp_type_Accountants, occyp_type_Cleaning staff, occyp_type_Cooking staff, occyp_type_Core staff, occyp_type_Drivers, occyp_type_HR staff, occyp_type_High skill tech staff, occyp_type_IT staff, occyp_type_Laborers, occyp_type_Low-skill Laborers, occyp_type_Managers, occyp_type_Medicine staff, occyp_type_NAN, occyp_type_Private service staff, occyp_type_Realty agents, occyp_type_Sales staff, occyp_type_Secretaries, occyp_type_Security staff, occyp_type_Waiters/barmen staff, family_bins_0, family_bins_1, family_bins_2]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 57 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "grateful-transfer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | lambda_l1 | lambda_l2 | max_depth | min_ch... | min_da... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input data must be 2 dimensional and non empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (0.009129410845395175, 0.023912339485083818, 13.21951832687341, 4268.6789634467705, 260.77112969775584, 3402.5570191469747)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-874a210c8ab2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m                                                 })\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mlgbBO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[0mxgboostBO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m \u001b[1;31m# 찾은 파라미터 값 확인\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-874a210c8ab2>\u001b[0m in \u001b[0;36mlgb_eval\u001b[1;34m(num_leaves, max_depth, lambda_l2, lambda_l1, min_child_samples, min_data_in_leaf)\u001b[0m\n\u001b[0;32m     30\u001b[0m                    \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                    \u001b[0mstratified\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                    nfold=10)\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_loss-mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, fpreproc, verbose_eval, show_stdv, seed, callbacks, eval_train_metric, return_cvbooster)\u001b[0m\n\u001b[0;32m    561\u001b[0m                             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpreproc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfpreproc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                             \u001b[0mstratified\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratified\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m                             eval_train_metric=eval_train_metric)\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[1;31m# setup callbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36m_make_n_folds\u001b[1;34m(full_data, folds, nfold, params, seed, fpreproc, stratified, shuffle, eval_train_metric)\u001b[0m\n\u001b[0;32m    319\u001b[0m                   shuffle=True, eval_train_metric=False):\n\u001b[0;32m    320\u001b[0m     \u001b[1;34m\"\"\"Make a n-fold list of Booster from random indices.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m     \u001b[0mfull_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m     \u001b[0mnum_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfolds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1323\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1325\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1063\u001b[0m                                                                                              \u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m                                                                                              \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m                                                                                              self.pandas_categorical)\n\u001b[0m\u001b[0;32m   1066\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_label_from_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input data must be 2 dimensional and non empty.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'auto'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfeature_name\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input data must be 2 dimensional and non empty."
     ]
    }
   ],
   "source": [
    "# Logloss 계산하는 함수 (fold=10, statified)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n",
    "    train = out_train\n",
    "    params = {\n",
    "        \"objective\" : \"multiclass\",\n",
    "        \"metric\" : \"log_loss\", \n",
    "        'is_unbalance': True,\n",
    "        \"num_leaves\" : int(num_leaves),\n",
    "        \"max_depth\" : int(max_depth),\n",
    "        \"lambda_l2\" : lambda_l2,\n",
    "        \"lambda_l1\" : lambda_l1,\n",
    "#         \"num_threads\" : 20,\n",
    "        \"min_child_samples\" : int(min_child_samples),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        \"learning_rate\" : 0.03,\n",
    "        \"subsample_freq\" : 5,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    \n",
    "    \n",
    "    lgtrain = lightgbm.Dataset(train.drop(['credit'],1), train['credit'])\n",
    "    cv_result = lightgbm.cv(params,\n",
    "                   lgtrain,\n",
    "                   1000,\n",
    "                   early_stopping_rounds=100,\n",
    "                   stratified=True,\n",
    "                   nfold=10)\n",
    "    return cv_result['log_loss-mean']\n",
    "    \n",
    "    \n",
    "#     n = 10\n",
    "#     scores = 0\n",
    "#     kfold = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "#     folds=[]\n",
    "#     for train_idx, valid_idx in kfold.split(train, train['credit']):\n",
    "#         folds.append((train_idx, valid_idx))\n",
    "#     models = []\n",
    "#     for fold in range(n):\n",
    "#         train_idx, valid_idx = folds[fold]\n",
    "#         X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "#                                              train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "        \n",
    "        \n",
    "#         model = LGBMClassifier(\n",
    "#             max_depth = int(max_depth),\n",
    "#             n_estimators = int(n_estimators), \n",
    "#             learning_rate = learning_rate,\n",
    "#             gamma = gamma,\n",
    "#             subsample = np.clip(subsample, 0, 1), \n",
    "#            num_leaves = int(num_leaves),\n",
    "#            min_child_samples = int(min_child_samples),\n",
    "#             reg_alpha = reg_alpha, \n",
    "#             reg_lambda = reg_lambda,\n",
    "#         )\n",
    "        \n",
    "#         model.fit(X_train, Y_train)\n",
    "\n",
    "#         Y_pred = model.predict_proba(X_valid)\n",
    "#         nlog_loss = log_loss(Y_valid,Y_pred)\n",
    "#         scores += nlog_loss\n",
    "#     return scores\n",
    "\n",
    "# # !pip install bayesian-optimization\n",
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
    "                                                'max_depth': (5, 63),\n",
    "                                                'lambda_l2': (0.0, 0.05),\n",
    "                                                'lambda_l1': (0.0, 0.05),\n",
    "                                                'min_child_samples': (50, 10000),\n",
    "                                                'min_data_in_leaf': (100, 2000)\n",
    "                                                })\n",
    "\n",
    "lgbBO.maximize(n_iter=10, init_points=2)\n",
    "\n",
    "xgboostBO.max # 찾은 파라미터 값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "meaning-harmony",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lgb_cv() missing 6 required positional arguments: 'n_estimators', 'learning_rate', 'subsample', 'colsample_bytree', 'reg_alpha', and 'reg_lambda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f1b64113b827>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m          \u001b[1;34m'reg_alpha'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m }\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mxgb_bayes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgb_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpbounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[0mxgb_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_bayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ei'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: lgb_cv() missing 6 required positional arguments: 'n_estimators', 'learning_rate', 'subsample', 'colsample_bytree', 'reg_alpha', and 'reg_lambda'"
     ]
    }
   ],
   "source": [
    "def lgb_cv(n_estimators, learning_rate, subsample, colsample_bytree, reg_alpha, reg_lambda, x_data=None, y_data=None, n=10, output='score'):\n",
    "    score = 0\n",
    "    kfold = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "    models = []\n",
    "    for train_index, valid_index in kf.split(x_data):\n",
    "        X_train, Y_train = x_data.iloc[train_index], y_data[train_index]\n",
    "        X_valid, Y_valid = x_data.iloc[valid_index], y_data[valid_index]\n",
    "        \n",
    "        \n",
    "        model = LGBMClassifier(\n",
    "            n_estimators = int(n_estimators), \n",
    "            learning_rate = learning_rate,\n",
    "            subsample = np.clip(subsample, 0, 1), \n",
    "            colsample_bytree = np.clip(colsample_bytree, 0, 1), \n",
    "            reg_alpha = reg_alpha, \n",
    "            reg_lambda = reg_lambda,\n",
    "        )\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        models.append(model)\n",
    "        \n",
    "        Y_pred = model.predict_proba(X_valid)\n",
    "        nlog_loss = log_loss(Y_valid,Y_pred)\n",
    "    if output == 'score':\n",
    "        return nlog_loss\n",
    "    if output == 'model':\n",
    "        return models\n",
    "\n",
    "\n",
    "\n",
    "# bayesian optimization\n",
    "#xgb_sk = XGBClassifier()\n",
    "params ={'n_estimators' : (1000,8000),\n",
    "         'max_depth' : (5,80),\n",
    "         'gamma' : (0,1),\n",
    "         'learnig_rate':(0.01, 0.05),\n",
    "         'min_child_weight':(2,5),\n",
    "         'subsample':(0.4,1),\n",
    "         'colsample_bytree':(0.4,1),\n",
    "         'reg_alpha': (1e-5,100)\n",
    "}\n",
    "xgb_bayes = BayesianOptimization(lgb_cv(), pbounds = params, random_state=42)\n",
    "xgb_max = xgb_bayes.maximize(init_points=5, n_iter=20, acq='ei')\n",
    "print(xgb_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spoken-sending",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================1============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.617011\tvalid_1's multi_logloss: 0.74062\n",
      "[200]\ttraining's multi_logloss: 0.521674\tvalid_1's multi_logloss: 0.733707\n",
      "Early stopping, best iteration is:\n",
      "[211]\ttraining's multi_logloss: 0.51266\tvalid_1's multi_logloss: 0.732927\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================2============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.620542\tvalid_1's multi_logloss: 0.743797\n",
      "[200]\ttraining's multi_logloss: 0.51959\tvalid_1's multi_logloss: 0.73226\n",
      "Early stopping, best iteration is:\n",
      "[254]\ttraining's multi_logloss: 0.478933\tvalid_1's multi_logloss: 0.730607\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================3============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.616593\tvalid_1's multi_logloss: 0.742441\n",
      "[200]\ttraining's multi_logloss: 0.51571\tvalid_1's multi_logloss: 0.728027\n",
      "Early stopping, best iteration is:\n",
      "[260]\ttraining's multi_logloss: 0.471315\tvalid_1's multi_logloss: 0.726384\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================4============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.616103\tvalid_1's multi_logloss: 0.746674\n",
      "[200]\ttraining's multi_logloss: 0.518713\tvalid_1's multi_logloss: 0.730005\n",
      "[300]\ttraining's multi_logloss: 0.448392\tvalid_1's multi_logloss: 0.726928\n",
      "Early stopping, best iteration is:\n",
      "[308]\ttraining's multi_logloss: 0.44337\tvalid_1's multi_logloss: 0.72658\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================5============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.613669\tvalid_1's multi_logloss: 0.743526\n",
      "[200]\ttraining's multi_logloss: 0.5173\tvalid_1's multi_logloss: 0.728229\n",
      "Early stopping, best iteration is:\n",
      "[244]\ttraining's multi_logloss: 0.483558\tvalid_1's multi_logloss: 0.72646\n",
      "================================================================================\n",
      "\n",
      "\n",
      "0.7285915982006854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds=[]\n",
    "losses=[]\n",
    "for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "\n",
    "    \n",
    "    \n",
    "lgb_models={}\n",
    "for fold in range(n):\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                         train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "    lgb = LGBMClassifier(n_estimators=1000)\n",
    "    lgb.fit(X_train, y_train, \n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "            early_stopping_rounds=30,\n",
    "           verbose=100)\n",
    "    lgb_models[fold]=lgb\n",
    "    \n",
    "    \n",
    "    \n",
    "    losses.append(log_loss(y_valid, lgb.predict_proba(X_valid)))\n",
    "print(sum(losses)/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-istanbul",
   "metadata": {},
   "source": [
    "# #######모델링, 학습 끝#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_zeros = np.zeros([len(in_test),3])\n",
    "for fold in range(n):\n",
    "    in_zeros += in_models[fold].predict_proba(in_test)/n\n",
    "in_output = pd.DataFrame(in_zeros)\n",
    "in_output = in_output.reindex(index=pd.Index(idx_in_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_zeros = np.zeros([len(out_test),3])\n",
    "for fold in range(n):\n",
    "    out_zeros += out_models[fold].predict_proba(out_test)/n\n",
    "out_output = pd.DataFrame(out_zeros)\n",
    "out_output = out_output.reindex(index=pd.Index(idx_out_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "in2 = np.concatenate((in_zeros,np.array(idx_in_test).reshape(-1,1)),axis=1)\n",
    "out2 = np.concatenate((out_zeros,np.array(idx_out_test).reshape(-1,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(np.concatenate((in2,out2),axis=0),columns=[0,1,2,'index'])\n",
    "output['index'] = output['index'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(output)):\n",
    "    row = output.loc[output['index']==i]\n",
    "    ss.iloc[i,1:] = row.iloc[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('two_models.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
