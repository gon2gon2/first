{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/dreamquark-ai/tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ii9KxdXvhbjM",
    "outputId": "54d39637-036d-4c16-ae1b-15557ebbad01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at ./content/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('./content/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHds-WztpXqE",
    "outputId": "ddfea470-3589-49bc-dd2c-c0fd3e1f19a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_tabnet\n",
      "  Cloning https://github.com/dreamquark-ai/tabnet.git (to revision develop) to /tmp/pip-install-gx_qav4y/pytorch-tabnet\n",
      "  Running command git clone -q https://github.com/dreamquark-ai/tabnet.git /tmp/pip-install-gx_qav4y/pytorch-tabnet\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied, skipping upgrade: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.8.1+cu101)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch_tabnet) (3.7.4.3)\n",
      "Building wheels for collected packages: pytorch-tabnet\n",
      "  Building wheel for pytorch-tabnet (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pytorch-tabnet: filename=pytorch_tabnet-3.1.1-cp37-none-any.whl size=39326 sha256=dbf370437df193bcfe8cb3ee932ec034b2624d8b3cea2c676b2862d1c401b1dc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-sz7wvvc3/wheels/a5/fe/e1/d7be493728d1fb7284583f5449d4cad80139ee994ef420f522\n",
      "Successfully built pytorch-tabnet\n",
      "Installing collected packages: pytorch-tabnet\n",
      "Successfully installed pytorch-tabnet-3.1.1\n",
      "Collecting bayesian-optimization\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
      "Building wheels for collected packages: bayesian-optimization\n",
      "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp37-none-any.whl size=11687 sha256=f633493b11ce07d5343c2f8299e737b5817a56f9cdeb7de7fef1017f93f88e6e\n",
      "  Stored in directory: /root/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n",
      "Successfully built bayesian-optimization\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install  \"git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade\n",
    "!pip install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--UXn99Ob9ou"
   },
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEkfFArsdd0w"
   },
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-20T01:33:23.103935Z",
     "iopub.status.busy": "2021-05-20T01:33:23.103614Z",
     "iopub.status.idle": "2021-05-20T01:33:23.435231Z",
     "shell.execute_reply": "2021-05-20T01:33:23.434382Z",
     "shell.execute_reply.started": "2021-05-20T01:33:23.103907Z"
    },
    "id": "XoG6tQf3hYRi",
    "outputId": "c3345ead-acfd-4d55-add9-114b4397870d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'sample_submission.csv', 'test.csv', 'train.csv', 'Untitled.ipynb']\n",
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import log_loss\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# import lightgbm\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "d = \"C:\\kaggle_data\\credit_card\"\n",
    "lst = os.listdir(d)\n",
    "print(lst)\n",
    "train = pd.read_csv(d + '\\\\' +lst[3])\n",
    "test = pd.read_csv(d + '\\\\' +lst[2])\n",
    "ss = pd.read_csv(d + '\\\\' +lst[1])\n",
    "train = train.drop(['index'], axis=1)\n",
    "train.fillna('NAN', inplace=True) \n",
    "test = test.drop(['index'], axis=1)\n",
    "test.fillna('NAN', inplace=True)\n",
    "\n",
    "# Married, Civil marriage\n",
    "train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= train['income_per_size'] * 2\n",
    "\n",
    "test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= test['income_per_size'] * 2\n",
    "\n",
    "def simple_marry(x):\n",
    "    if x == 'Married' or x =='Civil marriage':\n",
    "        return '0'\n",
    "    elif x == 'Separated' or x == 'Widow':\n",
    "        return '1'\n",
    "    else:\n",
    "        return '2'\n",
    "\n",
    "for df in [train,test]:\n",
    "    df['family_bins'] = df['family_type'].apply(simple_marry)\n",
    "\n",
    "# income_total을 로그변환\n",
    "train['income_total'] = np.log(train['income_total'])\n",
    "# train = train.drop('income_total',1)\n",
    "test['income_total'] = np.log(test['income_total'])\n",
    "# test = test.drop('income_total',1)\n",
    "\n",
    "# car와 reality를 합친 새로운 칼럼 careality\n",
    "train['car'] =train['car'].apply(lambda x: int(x=='Y'))\n",
    "train['reality'] =train['reality'].apply(lambda x: int(x=='Y'))\n",
    "test['car'] =test['car'].apply(lambda x: int(x=='Y'))\n",
    "test['reality'] =test['reality'].apply(lambda x: int(x=='Y'))\n",
    "\n",
    "train['careality'] = train['car'] + train['reality']\n",
    "train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "test['careality'] = test['car'] + test['reality']\n",
    "test = test.drop(['car', 'reality'],1)\n",
    "\n",
    "train.drop('gender',1,inplace=True)\n",
    "test.drop('gender',1,inplace=True)\n",
    "\n",
    "object_col = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "###############################\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train.drop(object_col, axis=1, inplace=True)\n",
    "train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test.drop(object_col, axis=1, inplace=True)\n",
    "test = pd.concat([test, test_onehot_df], axis=1)\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 제곱\n",
    "for df in [train,test]:\n",
    "    df['income_per_size'] = df['income_per_size'].apply(lambda x: x**2)\n",
    "    \n",
    "c = 'income_per_size'\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'income_total'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'begin_month'\n",
    "k = 2.2\n",
    "\n",
    "train[c] = train[c].apply(lambda x: x**2)\n",
    "test[c] = test[c].apply(lambda x: x**2)\n",
    "\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "# train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "######################\n",
    "c = 'DAYS_BIRTH'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "#기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "c = 'DAYS_EMPLOYED'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "ran = ['child_num', 'DAYS_BIRTH', 'DAYS_EMPLOYED','family_size', 'begin_month', 'income_per_size']\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "tf = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution='normal')\n",
    "tf.fit(train[ran])\n",
    "train[ran] = tf.transform(train[ran])\n",
    "test[ran] = tf.transform(test[ran])\n",
    "\n",
    "\n",
    "\n",
    "MAX_EPOCHS = 1000\n",
    "BATCH_SIZE = 1024\n",
    "VIRTUAL_BATCH_SIZE = 128\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Using {}\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T01:39:07.185724Z",
     "iopub.status.busy": "2021-05-20T01:39:07.185405Z",
     "iopub.status.idle": "2021-05-20T01:39:07.207830Z",
     "shell.execute_reply": "2021-05-20T01:39:07.207122Z",
     "shell.execute_reply.started": "2021-05-20T01:39:07.185697Z"
    },
    "id": "GyVKBBtqhYRn"
   },
   "outputs": [],
   "source": [
    "# cat_idxs = [ i for i, f in enumerate(train.columns) if f in object_col]\n",
    "\n",
    "# cat_dims = [len(total.iloc[:,idx].unique()) for idx in cat_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T01:55:30.479499Z",
     "iopub.status.busy": "2021-05-20T01:55:30.479106Z",
     "iopub.status.idle": "2021-05-20T01:55:30.493517Z",
     "shell.execute_reply": "2021-05-20T01:55:30.492578Z",
     "shell.execute_reply.started": "2021-05-20T01:55:30.479462Z"
    },
    "id": "cLqQ9HN9hYRn"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "folds=[]\n",
    "for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_step: 4,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▌                                | 1/10 [00:55<08:16, 55.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 180 with best_epoch = 130 and best_val_0_logloss = 0.85963\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 261 with best_epoch = 211 and best_val_0_logloss = 0.82056\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████▊                         | 3/10 [03:01<06:53, 59.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 153 with best_epoch = 103 and best_val_0_logloss = 0.8214\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 163 with best_epoch = 113 and best_val_0_logloss = 0.79653\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████▍                     | 4/10 [03:51<05:32, 55.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 227 with best_epoch = 177 and best_val_0_logloss = 0.83404\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████                  | 5/10 [05:00<05:01, 60.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 121 with best_epoch = 71 and best_val_0_logloss = 0.8613\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████▏          | 7/10 [06:15<02:23, 47.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 125 with best_epoch = 75 and best_val_0_logloss = 0.83562\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 189 with best_epoch = 139 and best_val_0_logloss = 0.82744\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████▊       | 8/10 [07:13<01:41, 50.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 159 with best_epoch = 109 and best_val_0_logloss = 0.84295\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████▍   | 9/10 [08:02<00:50, 50.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 126 with best_epoch = 76 and best_val_0_logloss = 0.85715\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10/10 [08:43<00:00, 52.30s/it]\n",
      "  0%|                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps가 3 일 때\n",
      "0.8356617237430285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███▌                                | 1/10 [01:00<09:00, 60.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_logloss = 0.81209\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 171 with best_epoch = 121 and best_val_0_logloss = 0.81331\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████▊                         | 3/10 [03:08<07:30, 64.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 213 with best_epoch = 163 and best_val_0_logloss = 0.81438\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████▍                     | 4/10 [03:49<05:30, 55.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 121 with best_epoch = 71 and best_val_0_logloss = 0.7988\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 126 with best_epoch = 76 and best_val_0_logloss = 0.80812\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████                  | 5/10 [04:31<04:12, 50.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 271 with best_epoch = 221 and best_val_0_logloss = 0.79768\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████▌              | 6/10 [06:02<04:16, 64.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 167 with best_epoch = 117 and best_val_0_logloss = 0.81688\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████▏          | 7/10 [06:58<03:03, 61.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 113 with best_epoch = 63 and best_val_0_logloss = 0.81937\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████▊       | 8/10 [07:35<01:47, 53.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 225 with best_epoch = 175 and best_val_0_logloss = 0.80847\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████▍   | 9/10 [08:50<01:00, 60.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 182 with best_epoch = 132 and best_val_0_logloss = 0.80271\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10/10 [09:51<00:00, 59.14s/it]\n",
      "  0%|                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps가 4 일 때\n",
      "0.8091813736413892\n",
      "\n",
      "Early stopping occurred at epoch 205 with best_epoch = 155 and best_val_0_logloss = 0.80297\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███▌                                | 1/10 [01:14<11:13, 74.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 178 with best_epoch = 128 and best_val_0_logloss = 0.80492\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████▏                            | 2/10 [02:20<09:15, 69.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 155 with best_epoch = 105 and best_val_0_logloss = 0.81611\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████▊                         | 3/10 [03:17<07:25, 63.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 179 with best_epoch = 129 and best_val_0_logloss = 0.79398\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████                  | 5/10 [05:28<05:24, 64.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 180 with best_epoch = 130 and best_val_0_logloss = 0.79991\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 157 with best_epoch = 107 and best_val_0_logloss = 0.80614\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████▌              | 6/10 [06:27<04:10, 62.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 145 with best_epoch = 95 and best_val_0_logloss = 0.81458\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████▏          | 7/10 [07:21<03:00, 60.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 168 with best_epoch = 118 and best_val_0_logloss = 0.81674\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████▊       | 8/10 [08:24<02:01, 60.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 178 with best_epoch = 128 and best_val_0_logloss = 0.80712\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████▍   | 9/10 [09:30<01:02, 62.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 149 with best_epoch = 99 and best_val_0_logloss = 0.80852\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10/10 [10:25<00:00, 62.52s/it]\n",
      "  0%|                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps가 5 일 때\n",
      "0.8070980616897933\n",
      "\n",
      "Early stopping occurred at epoch 123 with best_epoch = 73 and best_val_0_logloss = 0.80765\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███▌                                | 1/10 [00:49<07:29, 49.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 114 with best_epoch = 64 and best_val_0_logloss = 0.80693\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████▏                            | 2/10 [01:35<06:20, 47.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 149 with best_epoch = 99 and best_val_0_logloss = 0.81231\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████▊                         | 3/10 [02:35<06:11, 53.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 148 with best_epoch = 98 and best_val_0_logloss = 0.79318\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████                  | 5/10 [05:25<06:17, 75.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 280 with best_epoch = 230 and best_val_0_logloss = 0.79233\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████▌              | 6/10 [06:19<04:33, 68.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 135 with best_epoch = 85 and best_val_0_logloss = 0.80736\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 152 with best_epoch = 102 and best_val_0_logloss = 0.81502\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████▊       | 8/10 [08:50<02:26, 73.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 225 with best_epoch = 175 and best_val_0_logloss = 0.81567\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████▍   | 9/10 [09:55<01:10, 70.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 160 with best_epoch = 110 and best_val_0_logloss = 0.81187\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|███████████████████████████████████| 10/10 [11:21<00:00, 75.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 213 with best_epoch = 163 and best_val_0_logloss = 0.80231\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10/10 [11:21<00:00, 68.17s/it]\n",
      "  0%|                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps가 6 일 때\n",
      "0.8064630093440025\n",
      "\n",
      "Early stopping occurred at epoch 148 with best_epoch = 98 and best_val_0_logloss = 0.8098\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███▌                                | 1/10 [01:04<09:43, 64.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 128 with best_epoch = 78 and best_val_0_logloss = 0.81108\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████▊                         | 3/10 [03:17<07:51, 67.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 169 with best_epoch = 119 and best_val_0_logloss = 0.8127\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 179 with best_epoch = 129 and best_val_0_logloss = 0.796\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████▍                     | 4/10 [04:34<07:06, 71.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 165 with best_epoch = 115 and best_val_0_logloss = 0.80673\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████▌              | 6/10 [06:59<04:47, 71.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 168 with best_epoch = 118 and best_val_0_logloss = 0.80096\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 250 with best_epoch = 200 and best_val_0_logloss = 0.80747\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████▏          | 7/10 [08:43<04:07, 82.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 240 with best_epoch = 190 and best_val_0_logloss = 0.81514\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████▊       | 8/10 [10:23<02:56, 88.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 217 with best_epoch = 167 and best_val_0_logloss = 0.82102\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████▍   | 9/10 [11:54<01:28, 88.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 152 with best_epoch = 102 and best_val_0_logloss = 0.82139\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10/10 [12:58<00:00, 77.81s/it]\n",
      "  0%|                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps가 7 일 때\n",
      "0.8102283744747375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███▌                                | 1/10 [00:59<08:59, 59.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 132 with best_epoch = 82 and best_val_0_logloss = 0.80662\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 234 with best_epoch = 184 and best_val_0_logloss = 0.80153\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████▊                         | 3/10 [03:49<08:56, 76.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 143 with best_epoch = 93 and best_val_0_logloss = 0.8147\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 151 with best_epoch = 101 and best_val_0_logloss = 0.79355\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████▍                     | 4/10 [04:57<07:19, 73.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 145 with best_epoch = 95 and best_val_0_logloss = 0.79915\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████▌              | 6/10 [07:03<04:27, 66.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 132 with best_epoch = 82 and best_val_0_logloss = 0.80655\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████▏          | 7/10 [08:49<03:58, 79.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 235 with best_epoch = 185 and best_val_0_logloss = 0.81196\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 140 with best_epoch = 90 and best_val_0_logloss = 0.81354\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████▍   | 9/10 [11:04<01:13, 73.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 158 with best_epoch = 108 and best_val_0_logloss = 0.81568\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 118 with best_epoch = 68 and best_val_0_logloss = 0.81058\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10/10 [11:57<00:00, 71.78s/it]\n",
      "  0%|                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps가 8 일 때\n",
      "0.8073859744073515\n",
      "\n",
      "Early stopping occurred at epoch 237 with best_epoch = 187 and best_val_0_logloss = 0.80395\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███▌                               | 1/10 [01:54<17:07, 114.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 146 with best_epoch = 96 and best_val_0_logloss = 0.80584\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████▏                            | 2/10 [03:05<11:49, 88.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 155 with best_epoch = 105 and best_val_0_logloss = 0.80188\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████▊                         | 3/10 [04:20<09:37, 82.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 274 with best_epoch = 224 and best_val_0_logloss = 0.7919\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████                     | 4/10 [06:32<10:12, 102.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 146 with best_epoch = 96 and best_val_0_logloss = 0.80172\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████                  | 5/10 [07:42<07:33, 90.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 274 with best_epoch = 224 and best_val_0_logloss = 0.79719\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████              | 6/10 [09:54<06:58, 104.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 230 with best_epoch = 180 and best_val_0_logloss = 0.8102\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████▌          | 7/10 [11:45<05:20, 106.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 194 with best_epoch = 144 and best_val_0_logloss = 0.81313\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████       | 8/10 [13:20<03:25, 102.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 195 with best_epoch = 145 and best_val_0_logloss = 0.81328\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████▌   | 9/10 [14:54<01:40, 100.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 167 with best_epoch = 117 and best_val_0_logloss = 0.80551\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10/10 [16:15<00:00, 97.59s/it]\n",
      "  0%|                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps가 9 일 때\n",
      "0.8044594439965482\n",
      "\n",
      "Early stopping occurred at epoch 191 with best_epoch = 141 and best_val_0_logloss = 0.80372\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███▌                                | 1/10 [01:38<14:47, 98.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 231 with best_epoch = 181 and best_val_0_logloss = 0.8047\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████                            | 2/10 [03:37<14:43, 110.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 129 with best_epoch = 79 and best_val_0_logloss = 0.81081\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████▊                         | 3/10 [04:44<10:34, 90.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 138 with best_epoch = 88 and best_val_0_logloss = 0.79631\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████▍                     | 4/10 [05:56<08:19, 83.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 306 with best_epoch = 256 and best_val_0_logloss = 0.79026\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████▌                 | 5/10 [08:34<09:11, 110.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 169 with best_epoch = 119 and best_val_0_logloss = 0.80295\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████              | 6/10 [10:03<06:52, 103.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 144 with best_epoch = 94 and best_val_0_logloss = 0.80995\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████▏          | 7/10 [11:19<04:42, 94.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 159 with best_epoch = 109 and best_val_0_logloss = 0.81057\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████▊       | 8/10 [12:42<03:01, 90.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 159 with best_epoch = 109 and best_val_0_logloss = 0.81\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████▍   | 9/10 [14:05<01:28, 88.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 165 with best_epoch = 115 and best_val_0_logloss = 0.80702\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10/10 [15:32<00:00, 93.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps가 10 일 때\n",
      "0.8046298276708608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# loss=0\n",
    "# models={}\n",
    "# historys={}\n",
    "results = []\n",
    "for n_step in [5,6,9]:\n",
    "    for ind in range(1,6):\n",
    "        for sha in range(1,6):\n",
    "            loss=0\n",
    "            models={}\n",
    "            historys={}\n",
    "            for fold in tqdm(range(n)):\n",
    "                train_idx, valid_idx = folds[fold]\n",
    "                X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                                  train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "                clf = TabNetClassifier(n_d=9, n_a=9,\n",
    "                                n_steps=n_step,\n",
    "                                gamma=1.006,\n",
    "                                n_independent=ind,\n",
    "                                n_shared=sha,\n",
    "            #                     momentum=0.3, \n",
    "            #                     clip_value=2., \n",
    "                                lambda_sparse=0.01994,\n",
    "                                seed=42,\n",
    "\n",
    "                                optimizer_fn=torch.optim.Adam,\n",
    "                                optimizer_params=dict(lr=1e-2), \n",
    "                                scheduler_params = {\"gamma\": 0.95,\n",
    "                                  \"step_size\": 20},\n",
    "                                scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                                device_name = 'auto',\n",
    "                                verbose=0)\n",
    "\n",
    "                history = clf.fit(\n",
    "                  X_train=X_train, y_train=y_train,\n",
    "                  eval_set=[(X_valid, y_valid)],\n",
    "                  eval_metric=['logloss'],\n",
    "                  max_epochs=MAX_EPOCHS ,\n",
    "                  patience=50, # please be patient ^^\n",
    "                  batch_size=32768,\n",
    "                  virtual_batch_size=16384,\n",
    "                  num_workers=0)\n",
    "                models[fold] = clf\n",
    "                historys[fold]= history\n",
    "                # y_pred = clf.predict_proba(X_valid.values)\n",
    "                y_pred = clf.predict_proba(X_valid)\n",
    "                loss += log_loss(y_valid,y_pred)\n",
    "                results.append([n_step,ind,sha])\n",
    "                print(n_step,ind,sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['child_num', 'income_total', 'DAYS_BIRTH', 'DAYS_EMPLOYED',\n",
       "       'FLAG_MOBIL', 'work_phone', 'phone', 'email', 'family_size',\n",
       "       'begin_month', 'credit', 'income_per_size', 'careality',\n",
       "       'income_type_Commercial associate', 'income_type_Pensioner',\n",
       "       'income_type_State servant', 'income_type_Student',\n",
       "       'income_type_Working', 'edu_type_Academic degree',\n",
       "       'edu_type_Higher education', 'edu_type_Incomplete higher',\n",
       "       'edu_type_Lower secondary', 'edu_type_Secondary / secondary special',\n",
       "       'family_type_Civil marriage', 'family_type_Married',\n",
       "       'family_type_Separated', 'family_type_Single / not married',\n",
       "       'family_type_Widow', 'house_type_Co-op apartment',\n",
       "       'house_type_House / apartment', 'house_type_Municipal apartment',\n",
       "       'house_type_Office apartment', 'house_type_Rented apartment',\n",
       "       'house_type_With parents', 'occyp_type_Accountants',\n",
       "       'occyp_type_Cleaning staff', 'occyp_type_Cooking staff',\n",
       "       'occyp_type_Core staff', 'occyp_type_Drivers', 'occyp_type_HR staff',\n",
       "       'occyp_type_High skill tech staff', 'occyp_type_IT staff',\n",
       "       'occyp_type_Laborers', 'occyp_type_Low-skill Laborers',\n",
       "       'occyp_type_Managers', 'occyp_type_Medicine staff', 'occyp_type_NAN',\n",
       "       'occyp_type_Private service staff', 'occyp_type_Realty agents',\n",
       "       'occyp_type_Sales staff', 'occyp_type_Secretaries',\n",
       "       'occyp_type_Security staff', 'occyp_type_Waiters/barmen staff',\n",
       "       'family_bins_0', 'family_bins_1', 'family_bins_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8033376689532075"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabNetClassifier(n_d=9, n_a=9, n_steps=5, gamma=1.006, cat_idxs=[], cat_dims=[], cat_emb_dim=1, n_independent=2, n_shared=2, epsilon=1e-15, momentum=0.02, lambda_sparse=0.01994, seed=42, clip_value=1, verbose=0, optimizer_fn=<class 'torch.optim.adam.Adam'>, optimizer_params={'lr': 0.01}, scheduler_fn=<class 'torch.optim.lr_scheduler.StepLR'>, scheduler_params={'gamma': 0.95, 'step_size': 20}, mask_type='sparsemax', input_dim=55, output_dim=3, device_name='auto')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGkKiHAldZm2"
   },
   "source": [
    "## drdrdr 모델링, 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ock5bFpTcWYh"
   },
   "outputs": [],
   "source": [
    "##### 파라미터 최적화 탐색 중\n",
    "# n_d, n_a == 9, 16\n",
    "# lambda_sparse == 0.0001, 0.1478, 0.01994 (9일 떄)\n",
    "# decision_step == \n",
    "##### \n",
    "n = 4\n",
    "skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "folds=[]\n",
    "for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "random.seed(42)\n",
    "\n",
    "max_epochs = 1000\n",
    "models={}\n",
    "def eval_tabnet(n_independent):\n",
    "  loss=0\n",
    "  for fold in range(n):\n",
    "      train_idx, valid_idx = folds[fold]\n",
    "      X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                          train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "      \n",
    "      clf = TabNetClassifier(n_d=9, n_a=9,\n",
    "                        n_steps=5,\n",
    "                        gamma=1.006,\n",
    "#                         n_independent=int(n_independent),\n",
    "                        #  n_shared=int(n_shared),\n",
    "                        # momentum=0.3, \n",
    "                        # clip_value=2., \n",
    "                             \n",
    "                        lambda_sparse=0.01994,\n",
    "                        seed=42,\n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=1e-2), \n",
    "                        scheduler_params = {\"gamma\": 0.95,\n",
    "                          \"step_size\": 20},\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                        device_name = 'auto',\n",
    "                        verbose=0)\n",
    "      \n",
    "      clf.fit(\n",
    "          X_train=X_train, y_train=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "      #     eval_name=None,\n",
    "          eval_metric=['logloss'],\n",
    "      #     loss_fn=None,\n",
    "          max_epochs=max_epochs ,\n",
    "          patience=50, # please be patient ^^\n",
    "          batch_size=1024,\n",
    "          virtual_batch_size=256,\n",
    "          num_workers=1,\n",
    "          )\n",
    "      models[fold] = clf\n",
    "      # y_pred = clf.predict_proba(X_valid.values)\n",
    "      y_pred = clf.predict_proba(X_valid)\n",
    "      loss += log_loss(y_valid,y_pred)\n",
    "  return -loss/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jua7j6nvBCbX"
   },
   "source": [
    "n_d9일떄 gamma찾는 중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDR79LVKnxTa",
    "outputId": "58652361-7fc1-4945-a760-9b9f133a5567"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | n_inde... |\n",
      "-------------------------------------\n",
      "\n",
      "Early stopping occurred at epoch 168 with best_epoch = 118 and best_val_0_logloss = 0.80698\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 168 with best_epoch = 118 and best_val_0_logloss = 0.80577\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 107 with best_epoch = 57 and best_val_0_logloss = 0.81582\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 328 with best_epoch = 278 and best_val_0_logloss = 0.80272\n",
      "Best weights from best epoch are automatically used!\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.8078  \u001b[0m | \u001b[0m 3.593   \u001b[0m |\n",
      "\n",
      "Early stopping occurred at epoch 124 with best_epoch = 74 and best_val_0_logloss = 0.81269\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 72 and best_val_0_logloss = 0.81283\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 226 with best_epoch = 176 and best_val_0_logloss = 0.81384\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 119 with best_epoch = 69 and best_val_0_logloss = 0.81367\n",
      "Best weights from best epoch are automatically used!\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.8133  \u001b[0m | \u001b[0m 2.803   \u001b[0m |\n",
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_logloss = 0.80624\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 87 and best_val_0_logloss = 0.80571\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 199 with best_epoch = 149 and best_val_0_logloss = 0.81717\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 166 and best_val_0_logloss = 0.8035\n",
      "Best weights from best epoch are automatically used!\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.8082  \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_logloss = 0.80624\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 87 and best_val_0_logloss = 0.80571\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 199 with best_epoch = 149 and best_val_0_logloss = 0.81717\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 166 and best_val_0_logloss = 0.8035\n",
      "Best weights from best epoch are automatically used!\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.8082  \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_logloss = 0.80624\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 87 and best_val_0_logloss = 0.80571\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 199 with best_epoch = 149 and best_val_0_logloss = 0.81717\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 166 and best_val_0_logloss = 0.8035\n",
      "Best weights from best epoch are automatically used!\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.8082  \u001b[0m | \u001b[0m 4.193   \u001b[0m |\n",
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_logloss = 0.80624\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 87 and best_val_0_logloss = 0.80571\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 199 with best_epoch = 149 and best_val_0_logloss = 0.81717\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 166 and best_val_0_logloss = 0.8035\n",
      "Best weights from best epoch are automatically used!\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.8082  \u001b[0m | \u001b[0m 4.277   \u001b[0m |\n",
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_logloss = 0.80624\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 87 and best_val_0_logloss = 0.80571\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 199 with best_epoch = 149 and best_val_0_logloss = 0.81717\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 166 and best_val_0_logloss = 0.8035\n",
      "Best weights from best epoch are automatically used!\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.8082  \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_logloss = 0.80624\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 87 and best_val_0_logloss = 0.80571\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 199 with best_epoch = 149 and best_val_0_logloss = 0.81717\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 166 and best_val_0_logloss = 0.8035\n",
      "Best weights from best epoch are automatically used!\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.8082  \u001b[0m | \u001b[0m 4.303   \u001b[0m |\n",
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_logloss = 0.80624\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 87 and best_val_0_logloss = 0.80571\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 199 with best_epoch = 149 and best_val_0_logloss = 0.81717\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 166 and best_val_0_logloss = 0.8035\n",
      "Best weights from best epoch are automatically used!\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.8082  \u001b[0m | \u001b[0m 4.348   \u001b[0m |\n",
      "\n",
      "Early stopping occurred at epoch 174 with best_epoch = 124 and best_val_0_logloss = 0.80624\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 87 and best_val_0_logloss = 0.80571\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 199 with best_epoch = 149 and best_val_0_logloss = 0.81717\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "lgbBO = BayesianOptimization(eval_tabnet, {'n_independent': (1,5),\n",
    "                                        # 'n_d': (9,16)\n",
    "                                        # 'min_data_in_leaf': (100, 2000),\n",
    "                                        \n",
    "                                          })\n",
    "lgbBO.maximize(n_iter=20, init_points=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "execution": {
     "iopub.execute_input": "2021-05-20T02:03:17.823537Z",
     "iopub.status.busy": "2021-05-20T02:03:17.823135Z",
     "iopub.status.idle": "2021-05-20T02:03:17.879327Z",
     "shell.execute_reply": "2021-05-20T02:03:17.876766Z",
     "shell.execute_reply.started": "2021-05-20T02:03:17.823504Z"
    },
    "id": "CBUszx1ohYRo",
    "outputId": "5f8201c2-27d0-4973-d837-3d814a464a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 번째\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0767c8eba79b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mvirtual_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         num_workers=1)\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# Apply predict epoch to all eval sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/abstract_model.py\u001b[0m in \u001b[0;36m_train_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtabnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mmasked_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_transformers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0msteps_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecifics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mglu_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers_left\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglu_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglu_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_tabnet/tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models={}\n",
    "for fold in range(n):\n",
    "  print(n+1,'번째')\n",
    "  train_idx, valid_idx = folds[fold]\n",
    "  X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                      train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "  \n",
    "  clf = TabNetClassifier(n_d=int(9), n_a=int(9),\n",
    "                    # n_steps=3,\n",
    "                    # gamma=1.0,# n_independent=2, n_shared=2,\n",
    "                    # lambda_sparse=1e-4,\n",
    "                    cat_idxs=cat_idxs,\n",
    "                      cat_dims=cat_dims,\n",
    "                      cat_emb_dim=1,\n",
    "                    seed=42,\n",
    "                    # momentum=0.3, \n",
    "                    # clip_value=2., \n",
    "                    optimizer_fn=torch.optim.Adam,\n",
    "                    optimizer_params=dict(lr=1e-2), \n",
    "                    scheduler_params = {\"gamma\": 0.95,\n",
    "                      \"step_size\": 20},\n",
    "                    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                    device_name = 'auto',\n",
    "                    verbose=0)\n",
    "  clf.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "    #     eval_name=None,\n",
    "        eval_metric=['logloss'],\n",
    "    #     loss_fn=None,\n",
    "        max_epochs=max_epochs ,\n",
    "        patience=50, # please be patient ^^\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=256,\n",
    "        num_workers=1)\n",
    "  models[fold] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4IhPlwJQF3q",
    "outputId": "c854cad8-051f-4383-8773-d9a108c5a439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 114 with best_epoch = 64 and best_val_0_logloss = 0.87283\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "clf = TabNetClassifier(n_d=int(9), n_a=int(9),\n",
    "                  # n_steps=3,\n",
    "                  # gamma=1.0,# n_independent=2, n_shared=2,\n",
    "                  # lambda_sparse=1e-4,\n",
    "                  cat_idxs=cat_idxs,\n",
    "                    cat_dims=cat_dims,\n",
    "                    cat_emb_dim=1,\n",
    "                  seed=42,\n",
    "                  # momentum=0.3, \n",
    "                  # clip_value=2., \n",
    "                  optimizer_fn=torch.optim.Adam,\n",
    "                  optimizer_params=dict(lr=1e-2), \n",
    "                  scheduler_params = {\"gamma\": 0.95,\n",
    "                    \"step_size\": 20},\n",
    "                  scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                  device_name = 'auto',\n",
    "                  verbose=0)\n",
    "clf.fit(\n",
    "      X_train=X_train, y_train=y_train,\n",
    "      eval_set=[(X_valid, y_valid)],\n",
    "  #     eval_name=None,\n",
    "      eval_metric=['logloss'],\n",
    "  #     loss_fn=None,\n",
    "      max_epochs=max_epochs ,\n",
    "      patience=50, # please be patient ^^\n",
    "      batch_size=1024,\n",
    "      virtual_batch_size=256,\n",
    "      num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yVfa0sGQH8O",
    "outputId": "0d1a377f-d28d-4f3b-881f-8fe5e448fc18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19809672, 0.08804115, 0.7138621 ],\n",
       "       [0.24417205, 0.07967963, 0.6761483 ],\n",
       "       [0.13098761, 0.17606707, 0.69294536],\n",
       "       ...,\n",
       "       [0.11808927, 0.12027843, 0.76163226],\n",
       "       [0.06839902, 0.15826786, 0.7733332 ],\n",
       "       [0.12993054, 0.3628431 , 0.5072264 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict_proba(test.values)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "jELJ6wbIQfXx",
    "outputId": "8c0d5360-5f9b-4fac-e694-b3c76c515310"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26457</td>\n",
       "      <td>0.198097</td>\n",
       "      <td>0.088041</td>\n",
       "      <td>0.713862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26458</td>\n",
       "      <td>0.244172</td>\n",
       "      <td>0.079680</td>\n",
       "      <td>0.676148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26459</td>\n",
       "      <td>0.130988</td>\n",
       "      <td>0.176067</td>\n",
       "      <td>0.692945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26460</td>\n",
       "      <td>0.059098</td>\n",
       "      <td>0.189698</td>\n",
       "      <td>0.751205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26461</td>\n",
       "      <td>0.186102</td>\n",
       "      <td>0.133101</td>\n",
       "      <td>0.680797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>36452</td>\n",
       "      <td>0.169928</td>\n",
       "      <td>0.310359</td>\n",
       "      <td>0.519713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>36453</td>\n",
       "      <td>0.267642</td>\n",
       "      <td>0.247663</td>\n",
       "      <td>0.484696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>36454</td>\n",
       "      <td>0.118089</td>\n",
       "      <td>0.120278</td>\n",
       "      <td>0.761632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>36455</td>\n",
       "      <td>0.068399</td>\n",
       "      <td>0.158268</td>\n",
       "      <td>0.773333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>36456</td>\n",
       "      <td>0.129931</td>\n",
       "      <td>0.362843</td>\n",
       "      <td>0.507226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index         0         1         2\n",
       "0     26457  0.198097  0.088041  0.713862\n",
       "1     26458  0.244172  0.079680  0.676148\n",
       "2     26459  0.130988  0.176067  0.692945\n",
       "3     26460  0.059098  0.189698  0.751205\n",
       "4     26461  0.186102  0.133101  0.680797\n",
       "...     ...       ...       ...       ...\n",
       "9995  36452  0.169928  0.310359  0.519713\n",
       "9996  36453  0.267642  0.247663  0.484696\n",
       "9997  36454  0.118089  0.120278  0.761632\n",
       "9998  36455  0.068399  0.158268  0.773333\n",
       "9999  36456  0.129931  0.362843  0.507226\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.iloc[:,1:]=0\n",
    "ss.iloc[:,1:] += preds\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6ePna8GhYRo"
   },
   "outputs": [],
   "source": [
    "ss.iloc[:,1:]=0\n",
    "for fold in range(n):\n",
    "    ss.iloc[:,1:] += models[fold].predict_proba(test.values)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WOQJm4MJhYRp"
   },
   "outputs": [],
   "source": [
    "ss.to_csv('tabnet_test.csv', index=False) # 0.7272812144"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tabnet-test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
