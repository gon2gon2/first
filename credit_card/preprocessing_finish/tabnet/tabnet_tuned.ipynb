{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/dreamquark-ai/tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ii9KxdXvhbjM",
    "outputId": "54d39637-036d-4c16-ae1b-15557ebbad01"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-82db560a0e49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./content/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('./content/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHds-WztpXqE",
    "outputId": "ddfea470-3589-49bc-dd2c-c0fd3e1f19a5"
   },
   "outputs": [],
   "source": [
    "!pip install  \"git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade\n",
    "!pip install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--UXn99Ob9ou"
   },
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEkfFArsdd0w"
   },
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-20T01:33:23.103935Z",
     "iopub.status.busy": "2021-05-20T01:33:23.103614Z",
     "iopub.status.idle": "2021-05-20T01:33:23.435231Z",
     "shell.execute_reply": "2021-05-20T01:33:23.434382Z",
     "shell.execute_reply.started": "2021-05-20T01:33:23.103907Z"
    },
    "id": "XoG6tQf3hYRi",
    "outputId": "c3345ead-acfd-4d55-add9-114b4397870d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'sample_submission.csv', 'test.csv', 'train.csv', 'Untitled.ipynb']\n",
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import log_loss\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# import lightgbm\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "d = \"C:\\kaggle_data\\credit_card\"\n",
    "lst = os.listdir(d)\n",
    "print(lst)\n",
    "train = pd.read_csv(d + '\\\\' +lst[3])\n",
    "test = pd.read_csv(d + '\\\\' +lst[2])\n",
    "ss = pd.read_csv(d + '\\\\' +lst[1])\n",
    "train = train.drop(['index'], axis=1)\n",
    "train.fillna('NAN', inplace=True) \n",
    "test = test.drop(['index'], axis=1)\n",
    "test.fillna('NAN', inplace=True)\n",
    "\n",
    "# Married, Civil marriage\n",
    "train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= train['income_per_size'] * 2\n",
    "\n",
    "test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= test['income_per_size'] * 2\n",
    "\n",
    "def simple_marry(x):\n",
    "    if x == 'Married' or x =='Civil marriage':\n",
    "        return '0'\n",
    "    elif x == 'Separated' or x == 'Widow':\n",
    "        return '1'\n",
    "    else:\n",
    "        return '2'\n",
    "\n",
    "for df in [train,test]:\n",
    "    df['family_bins'] = df['family_type'].apply(simple_marry)\n",
    "\n",
    "# income_total을 로그변환\n",
    "train['income_total'] = np.log(train['income_total'])\n",
    "# train = train.drop('income_total',1)\n",
    "test['income_total'] = np.log(test['income_total'])\n",
    "# test = test.drop('income_total',1)\n",
    "\n",
    "# car와 reality를 합친 새로운 칼럼 careality\n",
    "train['car'] =train['car'].apply(lambda x: int(x=='Y'))\n",
    "train['reality'] =train['reality'].apply(lambda x: int(x=='Y'))\n",
    "test['car'] =test['car'].apply(lambda x: int(x=='Y'))\n",
    "test['reality'] =test['reality'].apply(lambda x: int(x=='Y'))\n",
    "\n",
    "train['careality'] = train['car'] + train['reality']\n",
    "train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "test['careality'] = test['car'] + test['reality']\n",
    "test = test.drop(['car', 'reality'],1)\n",
    "\n",
    "train.drop('gender',1,inplace=True)\n",
    "test.drop('gender',1,inplace=True)\n",
    "\n",
    "object_col = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "###############################\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train.drop(object_col, axis=1, inplace=True)\n",
    "train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test.drop(object_col, axis=1, inplace=True)\n",
    "test = pd.concat([test, test_onehot_df], axis=1)\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 제곱\n",
    "for df in [train,test]:\n",
    "    df['income_per_size'] = df['income_per_size'].apply(lambda x: x**2)\n",
    "    \n",
    "c = 'income_per_size'\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'income_total'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'begin_month'\n",
    "k = 2.2\n",
    "\n",
    "train[c] = train[c].apply(lambda x: x**2)\n",
    "test[c] = test[c].apply(lambda x: x**2)\n",
    "\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "# train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "######################\n",
    "c = 'DAYS_BIRTH'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "#기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "c = 'DAYS_EMPLOYED'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "ran = ['child_num', 'DAYS_BIRTH', 'DAYS_EMPLOYED','family_size', 'begin_month', 'income_per_size']\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "tf = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution='normal')\n",
    "tf.fit(train[ran])\n",
    "train[ran] = tf.transform(train[ran])\n",
    "test[ran] = tf.transform(test[ran])\n",
    "\n",
    "\n",
    "\n",
    "MAX_EPOCHS = 1000\n",
    "BATCH_SIZE = 1024\n",
    "VIRTUAL_BATCH_SIZE = 128\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Using {}\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T01:39:07.185724Z",
     "iopub.status.busy": "2021-05-20T01:39:07.185405Z",
     "iopub.status.idle": "2021-05-20T01:39:07.207830Z",
     "shell.execute_reply": "2021-05-20T01:39:07.207122Z",
     "shell.execute_reply.started": "2021-05-20T01:39:07.185697Z"
    },
    "id": "GyVKBBtqhYRn"
   },
   "outputs": [],
   "source": [
    "# cat_idxs = [ i for i, f in enumerate(train.columns) if f in object_col]\n",
    "\n",
    "# cat_dims = [len(total.iloc[:,idx].unique()) for idx in cat_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-20T01:55:30.479499Z",
     "iopub.status.busy": "2021-05-20T01:55:30.479106Z",
     "iopub.status.idle": "2021-05-20T01:55:30.493517Z",
     "shell.execute_reply": "2021-05-20T01:55:30.492578Z",
     "shell.execute_reply.started": "2021-05-20T01:55:30.479462Z"
    },
    "id": "cLqQ9HN9hYRn"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "folds=[]\n",
    "for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_step: 4,6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 튜닝 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models={}\n",
    "# for fold in range(n):\n",
    "#     train_idx, valid_idx = folds[fold]\n",
    "#     X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "#                                       train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "\n",
    "#     clf = TabNetClassifier(n_d=9, n_a=9,\n",
    "#                                     n_steps=6,\n",
    "#                                     gamma=1.006,\n",
    "#                                     n_independent=4,\n",
    "#                                     n_shared=4,\n",
    "#                                     lambda_sparse=0.01994,\n",
    "#                                     seed=42,\n",
    "\n",
    "#                                     optimizer_fn=torch.optim.Adam,\n",
    "#                                     optimizer_params=dict(lr=1e-2), \n",
    "#                                     scheduler_params = {\"gamma\": 0.95,\n",
    "#                                       \"step_size\": 20},\n",
    "#                                     scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "#                                     device_name = 'auto',\n",
    "#                                     verbose=0)\n",
    "#     clf.fit(\n",
    "#                   X_train=X_train, y_train=y_train,\n",
    "#                   eval_set=[(X_valid, y_valid)],\n",
    "#                   eval_metric=['logloss'],\n",
    "#                   max_epochs=MAX_EPOCHS ,\n",
    "#                   patience=50, # please be patient ^^\n",
    "#                   batch_size=32768,\n",
    "#                   virtual_batch_size=16384,\n",
    "#                   num_workers=0)\n",
    "#     models[fold] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={}\n",
    "for fold in range(n):\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                      train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "\n",
    "    clf = TabNetClassifier(n_d=9, n_a=9,\n",
    "                                    n_steps=6,\n",
    "                                    gamma=1.006,\n",
    "                                    n_independent=4,\n",
    "                                    n_shared=4,\n",
    "                                    lambda_sparse=0.01994,\n",
    "                                    seed=42,\n",
    "\n",
    "                                    optimizer_fn=torch.optim.Adam,\n",
    "                                    optimizer_params=dict(lr=1e-2), \n",
    "                                    scheduler_params = {\"gamma\": 0.95,\n",
    "                                      \"step_size\": 20},\n",
    "                                    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                                    device_name = 'auto',\n",
    "                                    verbose=0)\n",
    "    clf.fit(\n",
    "                  X_train=X_train, y_train=y_train,\n",
    "                  eval_set=[(X_valid, y_valid)],\n",
    "                  eval_metric=['logloss'],\n",
    "                  max_epochs=MAX_EPOCHS ,\n",
    "                  patience=50, # please be patient ^^\n",
    "                  batch_size=32768,\n",
    "                  virtual_batch_size=16384,\n",
    "                  num_workers=0)\n",
    "    models[fold] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={}\n",
    "for fold in range(n):\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                      train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "\n",
    "    clf = TabNetClassifier(device_name = DEVICE,verbose=0)\n",
    "    clf.fit(\n",
    "                  X_train=X_train, y_train=y_train,\n",
    "                  eval_set=[(X_valid, y_valid)],\n",
    "                  eval_metric=['logloss'],\n",
    "                  max_epochs=MAX_EPOCHS ,\n",
    "                  patience=50, # please be patient ^^\n",
    "                  batch_size=32768,\n",
    "                  virtual_batch_size=16384,\n",
    "                  num_workers=0)\n",
    "    models[fold] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.iloc[:,1:]=0\n",
    "for fold in range(n):\n",
    "    ss.iloc[:,1:] += models[fold].predict_proba(test.values)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('05_22_tabnet.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# loss=0\n",
    "# models={}\n",
    "# historys={}\n",
    "results = []\n",
    "for n_step in [5,6,9]:\n",
    "    for ind in range(1,6):\n",
    "        for sha in range(1,6):\n",
    "            loss=0\n",
    "            models={}\n",
    "            historys={}\n",
    "            for fold in tqdm(range(n)):\n",
    "                train_idx, valid_idx = folds[fold]\n",
    "                X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                                  train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "                clf = TabNetClassifier(n_d=9, n_a=9,\n",
    "                                n_steps=6\n",
    "                                gamma=1.006,\n",
    "                                n_independent=4,\n",
    "                                n_shared=4,\n",
    "                                lambda_sparse=0.01994,\n",
    "                                seed=42,\n",
    "\n",
    "                                optimizer_fn=torch.optim.Adam,\n",
    "                                optimizer_params=dict(lr=1e-2), \n",
    "                                scheduler_params = {\"gamma\": 0.95,\n",
    "                                  \"step_size\": 20},\n",
    "                                scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                                device_name = 'auto',\n",
    "                                verbose=0)\n",
    "\n",
    "                history = clf.fit(\n",
    "                  X_train=X_train, y_train=y_train,\n",
    "                  eval_set=[(X_valid, y_valid)],\n",
    "                  eval_metric=['logloss'],\n",
    "                  max_epochs=MAX_EPOCHS ,\n",
    "                  patience=50, # please be patient ^^\n",
    "                  batch_size=32768,\n",
    "                  virtual_batch_size=16384,\n",
    "                  num_workers=0)\n",
    "                models[fold] = clf\n",
    "                historys[fold]= history\n",
    "                # y_pred = clf.predict_proba(X_valid.values)\n",
    "                y_pred = clf.predict_proba(X_valid)\n",
    "                loss += log_loss(y_valid,y_pred)\n",
    "                results.append([n_step,ind,sha])\n",
    "                print(n_step,ind,sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGkKiHAldZm2"
   },
   "source": [
    "## drdrdr 모델링, 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ock5bFpTcWYh"
   },
   "outputs": [],
   "source": [
    "##### 파라미터 최적화 탐색 중\n",
    "# n_d, n_a == 9, 16\n",
    "# lambda_sparse == 0.0001, 0.1478, 0.01994 (9일 떄)\n",
    "# decision_step == \n",
    "##### \n",
    "n = 4\n",
    "skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "folds=[]\n",
    "for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "random.seed(42)\n",
    "\n",
    "max_epochs = 1000\n",
    "models={}\n",
    "def eval_tabnet(n_independent):\n",
    "  loss=0\n",
    "  for fold in range(n):\n",
    "      train_idx, valid_idx = folds[fold]\n",
    "      X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                          train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "      \n",
    "      clf = TabNetClassifier(n_d=9, n_a=9,\n",
    "                        n_steps=5,\n",
    "                        gamma=1.006,\n",
    "#                         n_independent=int(n_independent),\n",
    "                        #  n_shared=int(n_shared),\n",
    "                        # momentum=0.3, \n",
    "                        # clip_value=2., \n",
    "                             \n",
    "                        lambda_sparse=0.01994,\n",
    "                        seed=42,\n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=1e-2), \n",
    "                        scheduler_params = {\"gamma\": 0.95,\n",
    "                          \"step_size\": 20},\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                        device_name = 'auto',\n",
    "                        verbose=0)\n",
    "      \n",
    "      clf.fit(\n",
    "          X_train=X_train, y_train=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "      #     eval_name=None,\n",
    "          eval_metric=['logloss'],\n",
    "      #     loss_fn=None,\n",
    "          max_epochs=max_epochs ,\n",
    "          patience=50, # please be patient ^^\n",
    "          batch_size=1024,\n",
    "          virtual_batch_size=256,\n",
    "          num_workers=1,\n",
    "          )\n",
    "      models[fold] = clf\n",
    "      # y_pred = clf.predict_proba(X_valid.values)\n",
    "      y_pred = clf.predict_proba(X_valid)\n",
    "      loss += log_loss(y_valid,y_pred)\n",
    "  return -loss/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jua7j6nvBCbX"
   },
   "source": [
    "n_d9일떄 gamma찾는 중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDR79LVKnxTa",
    "outputId": "58652361-7fc1-4945-a760-9b9f133a5567",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lgbBO = BayesianOptimization(eval_tabnet, {'n_independent': (1,5),\n",
    "                                        # 'n_d': (9,16)\n",
    "                                        # 'min_data_in_leaf': (100, 2000),\n",
    "                                        \n",
    "                                          })\n",
    "lgbBO.maximize(n_iter=20, init_points=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "execution": {
     "iopub.execute_input": "2021-05-20T02:03:17.823537Z",
     "iopub.status.busy": "2021-05-20T02:03:17.823135Z",
     "iopub.status.idle": "2021-05-20T02:03:17.879327Z",
     "shell.execute_reply": "2021-05-20T02:03:17.876766Z",
     "shell.execute_reply.started": "2021-05-20T02:03:17.823504Z"
    },
    "id": "CBUszx1ohYRo",
    "outputId": "5f8201c2-27d0-4973-d837-3d814a464a25"
   },
   "outputs": [],
   "source": [
    "models={}\n",
    "for fold in range(n):\n",
    "  print(n+1,'번째')\n",
    "  train_idx, valid_idx = folds[fold]\n",
    "  X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                      train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "  \n",
    "  clf = TabNetClassifier(n_d=int(9), n_a=int(9),\n",
    "                    # n_steps=3,\n",
    "                    # gamma=1.0,# n_independent=2, n_shared=2,\n",
    "                    # lambda_sparse=1e-4,\n",
    "                    cat_idxs=cat_idxs,\n",
    "                      cat_dims=cat_dims,\n",
    "                      cat_emb_dim=1,\n",
    "                    seed=42,\n",
    "                    # momentum=0.3, \n",
    "                    # clip_value=2., \n",
    "                    optimizer_fn=torch.optim.Adam,\n",
    "                    optimizer_params=dict(lr=1e-2), \n",
    "                    scheduler_params = {\"gamma\": 0.95,\n",
    "                      \"step_size\": 20},\n",
    "                    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                    device_name = 'auto',\n",
    "                    verbose=0)\n",
    "  clf.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "    #     eval_name=None,\n",
    "        eval_metric=['logloss'],\n",
    "    #     loss_fn=None,\n",
    "        max_epochs=max_epochs ,\n",
    "        patience=50, # please be patient ^^\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=256,\n",
    "        num_workers=1)\n",
    "  models[fold] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4IhPlwJQF3q",
    "outputId": "c854cad8-051f-4383-8773-d9a108c5a439"
   },
   "outputs": [],
   "source": [
    "clf = TabNetClassifier(n_d=int(9), n_a=int(9),\n",
    "                  # n_steps=3,\n",
    "                  # gamma=1.0,# n_independent=2, n_shared=2,\n",
    "                  # lambda_sparse=1e-4,\n",
    "                  cat_idxs=cat_idxs,\n",
    "                    cat_dims=cat_dims,\n",
    "                    cat_emb_dim=1,\n",
    "                  seed=42,\n",
    "                  # momentum=0.3, \n",
    "                  # clip_value=2., \n",
    "                  optimizer_fn=torch.optim.Adam,\n",
    "                  optimizer_params=dict(lr=1e-2), \n",
    "                  scheduler_params = {\"gamma\": 0.95,\n",
    "                    \"step_size\": 20},\n",
    "                  scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                  device_name = 'auto',\n",
    "                  verbose=0)\n",
    "clf.fit(\n",
    "      X_train=X_train, y_train=y_train,\n",
    "      eval_set=[(X_valid, y_valid)],\n",
    "  #     eval_name=None,\n",
    "      eval_metric=['logloss'],\n",
    "  #     loss_fn=None,\n",
    "      max_epochs=max_epochs ,\n",
    "      patience=50, # please be patient ^^\n",
    "      batch_size=1024,\n",
    "      virtual_batch_size=256,\n",
    "      num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yVfa0sGQH8O",
    "outputId": "0d1a377f-d28d-4f3b-881f-8fe5e448fc18"
   },
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(test.values)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "jELJ6wbIQfXx",
    "outputId": "8c0d5360-5f9b-4fac-e694-b3c76c515310"
   },
   "outputs": [],
   "source": [
    "ss.iloc[:,1:]=0\n",
    "ss.iloc[:,1:] += preds\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6ePna8GhYRo"
   },
   "outputs": [],
   "source": [
    "ss.iloc[:,1:]=0\n",
    "for fold in range(n):\n",
    "    ss.iloc[:,1:] += models[fold].predict_proba(test.values)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOQJm4MJhYRp"
   },
   "outputs": [],
   "source": [
    "ss.to_csv('tabnet_test.csv', index=False) # 0.7272812144"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tabnet-test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
