{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beautiful-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import log_loss\n",
    "import torch\n",
    "\n",
    "d = \"C:\\kaggle_data\\credit_card\"\n",
    "\n",
    "train = pd.read_csv(d + '\\\\' +'train.csv')\n",
    "test = pd.read_csv(d + '\\\\' +'test.csv')\n",
    "ss = pd.read_csv(d + '\\\\' +'sample_submission.csv')\n",
    "train = train.drop(['index'], axis=1)\n",
    "train.fillna('NAN', inplace=True) \n",
    "test = test.drop(['index'], axis=1)\n",
    "test.fillna('NAN', inplace=True)\n",
    "\n",
    "y = train.pop('credit')\n",
    "\n",
    "def simple_pp(train, test):\n",
    "    \n",
    "    # Married, Civil marriage\n",
    "    train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "    test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "    train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "    = train['income_per_size'] * 2\n",
    "    test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "    = test['income_per_size'] * 2\n",
    "    \n",
    "    train['gender'] =train['gender'].apply(lambda x: int(x=='F'))\n",
    "    test['gender'] =test['gender'].apply(lambda x: int(x=='F'))\n",
    "    \n",
    "    \n",
    "    def simple_marry(x):\n",
    "        if x == 'Married' or x =='Civil marriage':\n",
    "            return '0'\n",
    "        elif x == 'Separated' or x == 'Widow':\n",
    "            return '1'\n",
    "        else:\n",
    "            return '2'\n",
    "\n",
    "    for df in [train,test]:\n",
    "        df['family_bins'] = df['family_type'].apply(simple_marry)\n",
    "\n",
    "    # car와 reality를 합친 새로운 칼럼 careality\n",
    "    train['car'] =train['car'].apply(lambda x: int(x=='Y'))\n",
    "    train['reality'] =train['reality'].apply(lambda x: int(x=='Y'))\n",
    "    test['car'] =test['car'].apply(lambda x: int(x=='Y'))\n",
    "    test['reality'] =test['reality'].apply(lambda x: int(x=='Y'))\n",
    "\n",
    "    train['careality'] = train['car'] + train['reality']\n",
    "    train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "    test['careality'] = test['car'] + test['reality']\n",
    "    test = test.drop(['car', 'reality'],1)\n",
    "\n",
    "\n",
    "\n",
    "    object_col = []\n",
    "    for col in train.columns:\n",
    "        if train[col].dtype == 'object':\n",
    "            object_col.append(col)\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "    train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "                 columns=enc.get_feature_names(object_col))\n",
    "    train.drop(object_col, axis=1, inplace=True)\n",
    "    train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "    test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "                 columns=enc.get_feature_names(object_col))\n",
    "    test.drop(object_col, axis=1, inplace=True)\n",
    "    test = pd.concat([test, test_onehot_df], axis=1)\n",
    "\n",
    "    ## 제곱\n",
    "    for df in [train,test]:\n",
    "        df['income_per_size'] = df['income_per_size'].apply(lambda x: x**2)\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "def log_pp(train,test):\n",
    "    \n",
    "    # Married, Civil marriage\n",
    "    train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "    test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "    train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "    = train['income_per_size'] * 2\n",
    "    test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "    = test['income_per_size'] * 2\n",
    "\n",
    "    def simple_marry(x):\n",
    "        if x == 'Married' or x =='Civil marriage':\n",
    "            return '0'\n",
    "        elif x == 'Separated' or x == 'Widow':\n",
    "            return '1'\n",
    "        else:\n",
    "            return '2'\n",
    "\n",
    "    for df in [train,test]:\n",
    "        df['family_bins'] = df['family_type'].apply(simple_marry)\n",
    "\n",
    "    # car와 reality를 합친 새로운 칼럼 careality\n",
    "    train['car'] =train['car'].apply(lambda x: int(x=='Y'))\n",
    "    train['reality'] =train['reality'].apply(lambda x: int(x=='Y'))\n",
    "    test['car'] =test['car'].apply(lambda x: int(x=='Y'))\n",
    "    test['reality'] =test['reality'].apply(lambda x: int(x=='Y'))\n",
    "\n",
    "    train['careality'] = train['car'] + train['reality']\n",
    "    train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "    test['careality'] = test['car'] + test['reality']\n",
    "    test = test.drop(['car', 'reality'],1)\n",
    "\n",
    "\n",
    "\n",
    "    object_col = []\n",
    "    for col in train.columns:\n",
    "        if train[col].dtype == 'object':\n",
    "            object_col.append(col)\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "    train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "                 columns=enc.get_feature_names(object_col))\n",
    "    train.drop(object_col, axis=1, inplace=True)\n",
    "    train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "    test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "                 columns=enc.get_feature_names(object_col))\n",
    "    test.drop(object_col, axis=1, inplace=True)\n",
    "    test = pd.concat([test, test_onehot_df], axis=1)\n",
    "\n",
    "    ## 제곱\n",
    "    for df in [train,test]:\n",
    "        df['income_per_size'] = df['income_per_size'].apply(lambda x: x**2)\n",
    "    \n",
    "    \n",
    "    log_cols = ['income_per_size', 'income_total', 'begin_month','DAYS_BIRTH']\n",
    "    \n",
    "    for col in log_cols:\n",
    "        train[col] = np.log1p(train[col].abs())\n",
    "        test[col] = np.log1p(test[col].abs())\n",
    "    \n",
    "    \n",
    "    train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'].abs()\n",
    "    test['DAYS_EMPLOYED'] = test['DAYS_EMPLOYED'].abs()\n",
    "        \n",
    "    m = train['DAYS_EMPLOYED'].mean()\n",
    "    s = train['DAYS_EMPLOYED'].std()\n",
    "        \n",
    "    for df in [train,test]:        \n",
    "        df['DAYS_EMPLOYED'] = (df['DAYS_EMPLOYED']-m)/s\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "def scale_pp(train,test):\n",
    "    \n",
    "    # Married, Civil marriage\n",
    "    train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "    test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "    train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "    = train['income_per_size'] * 2\n",
    "    test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "    = test['income_per_size'] * 2\n",
    "\n",
    "    def simple_marry(x):\n",
    "        if x == 'Married' or x =='Civil marriage':\n",
    "            return '0'\n",
    "        elif x == 'Separated' or x == 'Widow':\n",
    "            return '1'\n",
    "        else:\n",
    "            return '2'\n",
    "\n",
    "    for df in [train,test]:\n",
    "        df['family_bins'] = df['family_type'].apply(simple_marry)\n",
    "\n",
    "    # income_total을 로그변환\n",
    "    train['income_total'] = np.log(train['income_total'])\n",
    "    test['income_total'] = np.log(test['income_total'])\n",
    "\n",
    "\n",
    "    # car와 reality를 합친 새로운 칼럼 careality\n",
    "    train['car'] =train['car'].apply(lambda x: int(x=='Y'))\n",
    "    train['reality'] =train['reality'].apply(lambda x: int(x=='Y'))\n",
    "    test['car'] =test['car'].apply(lambda x: int(x=='Y'))\n",
    "    test['reality'] =test['reality'].apply(lambda x: int(x=='Y'))\n",
    "\n",
    "    train['careality'] = train['car'] + train['reality']\n",
    "    train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "    test['careality'] = test['car'] + test['reality']\n",
    "    test = test.drop(['car', 'reality'],1)\n",
    "\n",
    "\n",
    "\n",
    "    object_col = []\n",
    "    for col in train.columns:\n",
    "        if train[col].dtype == 'object':\n",
    "            object_col.append(col)\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "    train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "                 columns=enc.get_feature_names(object_col))\n",
    "    train.drop(object_col, axis=1, inplace=True)\n",
    "    train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "    test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "                 columns=enc.get_feature_names(object_col))\n",
    "    test.drop(object_col, axis=1, inplace=True)\n",
    "    test = pd.concat([test, test_onehot_df], axis=1)\n",
    "\n",
    "    ## 제곱\n",
    "    for df in [train,test]:\n",
    "        df['income_per_size'] = df['income_per_size'].apply(lambda x: x**2)\n",
    "    \n",
    "    \n",
    "    scale_cols = ['income_per_size', 'income_total', 'begin_month','DAYS_BIRTH','DAYS_EMPLOYED']\n",
    "    \n",
    "    for col in scale_cols:\n",
    "        m = train[col].mean()\n",
    "        s = train[col].std()\n",
    "        \n",
    "        train[col] = (train[col]-m)/s\n",
    "        test[col] = (test[col]-m)/s\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sized-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 1000\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Using {}\".format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indian-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 1000, \n",
    "                                    max_depth = 300, \n",
    "                                    min_samples_leaf = 2, \n",
    "                                    min_samples_split = 6,\n",
    "                                    max_features = 9)\n",
    "\n",
    "nb = GaussianNB()\n",
    "xgboost = XGBClassifier(#tree_method = \"gpu_hist\",\n",
    "                        n_estimators = 700,\n",
    "                        max_depth = 25,\n",
    "                        learning_rate= 0.1,\n",
    "                        min_child_weight = 2,\n",
    "                        subsample = 0.85,\n",
    "                        colsample_bytree = 0.31,\n",
    "                        gamma = 0,\n",
    "                        max_delta_step = 0.07,\n",
    "                        nthread = -1,\n",
    "                        eval_metric = 'mlogloss')\n",
    "\n",
    "knn_16 = KNeighborsClassifier(n_neighbors = 16, metric='euclidean', weights='distance')\n",
    "\n",
    "lightgbm = LGBMClassifier(boosting_type='gbdt',\n",
    "                          objective='multiclass',\n",
    "                          n_estimators=1000,\n",
    "                          max_depth = 9,\n",
    "                          subsample=0.9,\n",
    "                          subsample_freq=2,\n",
    "                          colsample_bytree=0.7,\n",
    "                          n_jobs=-1,\n",
    "                          eval_metric = 'mlogloss')\n",
    "\n",
    "tabnet = TabNetClassifier(n_d=9, n_a=9,\n",
    "                            n_steps=6,\n",
    "                            gamma=1.006,\n",
    "                            n_independent=4,\n",
    "                            n_shared=4,\n",
    "                            lambda_sparse=0.01994,\n",
    "                            seed=42,\n",
    "\n",
    "                            optimizer_fn=torch.optim.Adam,\n",
    "                            optimizer_params=dict(lr=1e-2), \n",
    "                            scheduler_params = {\"gamma\": 0.95,\n",
    "                              \"step_size\": 20},\n",
    "                            scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                            device_name = 'auto',\n",
    "                            verbose=0)\n",
    "\n",
    "\n",
    "models = [tabnet, forest, nb, xgboost, lightgbm, knn_16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "economic-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_train(model,X, y, test):\n",
    "    \"\"\"\n",
    "    모델, X, y, test를 넣어주면\n",
    "    oof로 만든 새로운 train과 test셋의 앙상블 결과를 반환\n",
    "    \n",
    "    X_oof: train을 oof로 학습, 예측하여 만든 새로운 train set\n",
    "    test_oof: skf로 학습한 총 10개의 모델 결과를 soft voting\n",
    "    \"\"\"\n",
    "    if model != tabnet:\n",
    "    \n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "        X_oof = np.zeros(y.shape)\n",
    "        ss = np.zeros((10000,3))\n",
    "    #     y_test = np.zeros(())\n",
    "        for train_idx, valid_idx in skf.split(X, y):\n",
    "            X_train, y_train = X.iloc[train_idx,:], y[train_idx]\n",
    "            X_valid, y_valid = X.iloc[valid_idx,:], y[valid_idx]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            X_oof[valid_idx] += model.predict(X.iloc[valid_idx])\n",
    "\n",
    "            ss += model.predict_proba(test)\n",
    "\n",
    "        test_oof = np.argmax(ss,1)\n",
    "\n",
    "\n",
    "        return X_oof, test_oof\n",
    "    \n",
    "    \n",
    "    elif model == tabnet:\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "        X_oof = np.zeros(y.shape)\n",
    "        ss = np.zeros((10000,3))\n",
    "    \n",
    "    \n",
    "        for train_idx, valid_idx in skf.split(X, y):\n",
    "            X_train, y_train = X.iloc[train_idx,:].values, y[train_idx].values\n",
    "            X_valid, y_valid = X.iloc[valid_idx,:].values, y[valid_idx].values\n",
    "\n",
    "            model = TabNetClassifier(n_d=9, n_a=9,\n",
    "                                    n_steps=6,\n",
    "                                    gamma=1.006,\n",
    "                                    n_independent=4,\n",
    "                                    n_shared=4,\n",
    "                                    lambda_sparse=0.01994,\n",
    "                                    seed=42,\n",
    "\n",
    "                                    optimizer_fn=torch.optim.Adam,\n",
    "                                    optimizer_params=dict(lr=1e-2), \n",
    "                                    scheduler_params = {\"gamma\": 0.95,\n",
    "                                      \"step_size\": 20},\n",
    "                                    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15,\n",
    "                                    device_name = 'auto',\n",
    "                                    verbose=0)      \n",
    "            \n",
    "            model.fit(X_train=X_train, y_train=y_train,\n",
    "                      eval_set=[(X_valid, y_valid)],\n",
    "                      eval_metric=['logloss'],\n",
    "                      max_epochs=MAX_EPOCHS ,\n",
    "                      patience=50, # please be patient ^^\n",
    "                      batch_size=32768,\n",
    "                      virtual_batch_size=16384)\n",
    "            \n",
    "            X_oof[valid_idx] += model.predict(X.iloc[valid_idx].values)\n",
    "            ss += model.predict_proba(test.values)\n",
    "\n",
    "        test_oof = np.argmax(ss,1)\n",
    "        \n",
    "        return X_oof, test_oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dated-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = simple_pp(train, test)\n",
    "\n",
    "# 연속형 로그 변환\n",
    "x_train_log, x_test_log = log_pp(train, test)\n",
    "\n",
    "# 연속형 표준화\n",
    "x_train_scale, x_test_scale = scale_pp(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-blame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNetClassifier의 1번째 학습중\n",
      "\n",
      "Early stopping occurred at epoch 148 with best_epoch = 98 and best_val_0_logloss = 0.83285\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 377 with best_epoch = 327 and best_val_0_logloss = 0.81246\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 481 with best_epoch = 431 and best_val_0_logloss = 0.81442\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 103 with best_epoch = 53 and best_val_0_logloss = 0.8623\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 144 with best_epoch = 94 and best_val_0_logloss = 0.85789\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 180 with best_epoch = 130 and best_val_0_logloss = 0.82315\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 162 with best_epoch = 112 and best_val_0_logloss = 0.82264\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 83 with best_epoch = 33 and best_val_0_logloss = 0.87409\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 111 with best_epoch = 61 and best_val_0_logloss = 0.86298\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 158 with best_epoch = 108 and best_val_0_logloss = 0.84974\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNetClassifier의 2번째 학습중\n",
      "\n",
      "Early stopping occurred at epoch 74 with best_epoch = 24 and best_val_0_logloss = 0.89739\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 65 with best_epoch = 15 and best_val_0_logloss = 0.89004\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 151 with best_epoch = 101 and best_val_0_logloss = 0.86848\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 396 with best_epoch = 346 and best_val_0_logloss = 0.81259\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 270 with best_epoch = 220 and best_val_0_logloss = 0.84319\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Early stopping occurred at epoch 69 with best_epoch = 19 and best_val_0_logloss = 0.8948\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "# 5:22 시작\n",
    "# 5:58 15 fold\n",
    "trains = [x_train, x_train_log, x_train_scale]\n",
    "tests = [x_test, x_test_log, x_test_scale]\n",
    "columns = []\n",
    "\n",
    "train_lv0 = []\n",
    "test_lv0 = []\n",
    "for model in models:\n",
    "    cnt = 0\n",
    "    model_name = str(model).split('(')[0]\n",
    "    for X, test in zip(trains, tests):\n",
    "        print(f'{model_name}의 {cnt+1}번째 학습중')\n",
    "        col_name = str(model).split('(')[0]+'_'+str(cnt+1)\n",
    "        \n",
    "        X_oof, test_oof = oof_train(model, X, y, test)\n",
    "        train_lv0.append(X_oof)\n",
    "        test_lv0.append(test_oof)\n",
    "        columns.append(col_name)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0 = pd.DataFrame(np.array(train_lv0).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-freeware",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_0 = pd.DataFrame(np.array(test_lv0).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0.to_csv('new_train.csv',index=False)\n",
    "test_0.to_csv('new_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "def nn(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(layers.Dense(8,activation='relu'))\n",
    "    model.add(layers.Dense(3,activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(monitor='sparse_categorical_crossentropy',patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "X = train_0\n",
    "\n",
    "nn = np.zeros((10000,3))\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(X,y)):    \n",
    "    \n",
    "    X_train, y_train = X.iloc[train_idx,:], y[train_idx]\n",
    "    X_valid, y_valid = X.iloc[valid_idx,:], y[valid_idx]\n",
    "    \n",
    "    model = nn(len(X.columns))\n",
    "    model.fit(x = X_train, \n",
    "                y = y_train,\n",
    "               epochs=10000,\n",
    "                batch_size=98304,\n",
    "               validation_data =(X_valid, y_valid),\n",
    "                   verbose=100,\n",
    "                   callbacks=[es])\n",
    "    \n",
    "    nn += model.predict_proba(test)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = np.zeros((10000,3))\n",
    "xgboost = XGBClassifier(tree_method = \"gpu_hist\",\n",
    "                        n_estimators = 700,\n",
    "                        max_depth = 25,\n",
    "                        learning_rate= 0.1,\n",
    "                        min_child_weight = 2,\n",
    "                        subsample = 0.85,\n",
    "                        colsample_bytree = 0.31,\n",
    "                        gamma = 0,\n",
    "                        max_delta_step = 0.07,\n",
    "                        nthread = -1,\n",
    "                        eval_metric = 'mlogloss')\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(X,y)):    \n",
    "    \n",
    "    X_train, y_train = X.iloc[train_idx,:], y[train_idx]\n",
    "    X_valid, y_valid = X.iloc[valid_idx,:], y[valid_idx]\n",
    "    \n",
    "    xgboost.fit(X_train,y_train,\n",
    "               eval_set = [(X_valid,y_valid)])\n",
    "    \n",
    "    xg += xgboost.predict_proba(test)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_xg = 0.5\n",
    "weight_nn = 0.5\n",
    "ss = pd.read_csv(d + '\\\\' +'sample_submission.csv')\n",
    "ss.iloc[:,1:]=0\n",
    "    ss.iloc[:,1:] += (weight_xg * xg) + (weight_nn * nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-groove",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
