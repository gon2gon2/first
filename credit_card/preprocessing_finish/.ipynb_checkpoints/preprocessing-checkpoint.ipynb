{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legal-nigeria",
   "metadata": {},
   "source": [
    "# \n",
    "- 현재의 전처리 과정을 거친 애들인데 데이터 반으로 쪼갠다\n",
    "- 베이스라인 \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "- 랜포 ,xgb\n",
    "- 탭넷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "quantitative-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "devoted-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'sample_submission.csv', 'test.csv', 'train.csv', 'Untitled.ipynb']\n"
     ]
    }
   ],
   "source": [
    "d = \"C:\\kaggle_data\\credit_card\"\n",
    "lst = os.listdir(d)\n",
    "print(lst)\n",
    "train = pd.read_csv(d + '\\\\' +lst[3])\n",
    "test = pd.read_csv(d + '\\\\' +lst[2])\n",
    "ss = pd.read_csv(d + '\\\\' +lst[1])\n",
    "train = train.drop(['index'], axis=1)\n",
    "train.fillna('NAN', inplace=True) \n",
    "test = test.drop(['index'], axis=1)\n",
    "test.fillna('NAN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "internal-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Married, Civil marriage\n",
    "train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= train['income_per_size'] * 2\n",
    "\n",
    "test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "= test['income_per_size'] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accomplished-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_marry(x):\n",
    "    if x == 'Married' or x =='Civil marriage':\n",
    "        return '0'\n",
    "    elif x == 'Separated' or x == 'Widow':\n",
    "        return '1'\n",
    "    else:\n",
    "        return '2'\n",
    "\n",
    "for df in [train,test]:\n",
    "    df['family_bins'] = df['family_type'].apply(simple_marry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "increasing-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# income_total을 로그변환\n",
    "train['income_total'] = np.log(train['income_total'])\n",
    "# train = train.drop('income_total',1)\n",
    "test['income_total'] = np.log(test['income_total'])\n",
    "# test = test.drop('income_total',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "constant-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# car와 reality를 합친 새로운 칼럼 careality\n",
    "train['car'] =train['car'].apply(lambda x: int(x=='Y'))\n",
    "train['reality'] =train['reality'].apply(lambda x: int(x=='Y'))\n",
    "test['car'] =test['car'].apply(lambda x: int(x=='Y'))\n",
    "test['reality'] =test['reality'].apply(lambda x: int(x=='Y'))\n",
    "\n",
    "train['careality'] = train['car'] + train['reality']\n",
    "train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "test['careality'] = test['car'] + test['reality']\n",
    "test = test.drop(['car', 'reality'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "accredited-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('gender',1,inplace=True)\n",
    "test.drop('gender',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "latest-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        object_col.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "informal-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train.drop(object_col, axis=1, inplace=True)\n",
    "train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test.drop(object_col, axis=1, inplace=True)\n",
    "test = pd.concat([test, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ambient-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 제곱\n",
    "for df in [train,test]:\n",
    "    df['income_per_size'] = df['income_per_size'].apply(lambda x: x**2)\n",
    "    \n",
    "c = 'income_per_size'\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'income_total'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "#####################\n",
    "c = 'begin_month'\n",
    "k = 2.2\n",
    "\n",
    "train[c] = train[c].apply(lambda x: x**2)\n",
    "test[c] = test[c].apply(lambda x: x**2)\n",
    "\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "# train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "######################\n",
    "c = 'DAYS_BIRTH'\n",
    "k = 2.2\n",
    "mean = train[c].mean()\n",
    "std = train[c].std()\n",
    "idxs = train.loc[(train[c]>= mean + k*std)|\\\n",
    "                (train[c]<= mean - k*std)].index\n",
    "train = train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "#기존 \n",
    "train[c] = (train[c]-mean)/std\n",
    "test[c] = (test[c]-mean)/std\n",
    "\n",
    "\n",
    "##########################\n",
    "# out: 아웃라이어\n",
    "out_train = train.loc[train.DAYS_EMPLOYED>0]\n",
    "out_test = test.loc[test.DAYS_EMPLOYED>0]\n",
    "\n",
    "in_train = train.loc[train.DAYS_EMPLOYED<=0]\n",
    "in_test = test.loc[test.DAYS_EMPLOYED<=0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "c = 'DAYS_EMPLOYED'\n",
    "k = 2.2\n",
    "mean = in_train[c].mean()\n",
    "std = in_train[c].std()\n",
    "idxs = in_train.loc[(in_train[c]>= mean + k*std)|\\\n",
    "                (in_train[c]<= mean - k*std)].index\n",
    "in_train = in_train.drop(idxs).reset_index(drop=True)\n",
    "\n",
    "# 기존 \n",
    "in_train[c] = (train[c]-mean)/std\n",
    "in_test[c] = (test[c]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "optical-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train = out_train.reset_index()\n",
    "in_train = in_train.reset_index()\n",
    "\n",
    "out_test = out_test.reset_index()\n",
    "in_test = in_test.reset_index()\n",
    "\n",
    "idx_out_train = out_train.pop('index')\n",
    "idx_out_test = out_test.pop('index')\n",
    "\n",
    "idx_in_train = in_train.pop('index')\n",
    "idx_in_test = in_test.pop('index')\n",
    "\n",
    "out_train.drop('DAYS_EMPLOYED',inplace=True,axis=1)\n",
    "out_test.drop('DAYS_EMPLOYED',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-dating",
   "metadata": {},
   "source": [
    "# #######모델링, 학습 시작#######\n",
    "\n",
    "|365243?|학습|예측|\n",
    "|----|----|----|\n",
    "|yes|out_train|out_test|\n",
    "|no|in_train|in_test|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-perception",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "commercial-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "academic-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-actor",
   "metadata": {},
   "source": [
    "### out_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "spoken-sending",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tr(train):\n",
    "    skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "    folds=[]\n",
    "    losses=[]\n",
    "    for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "        folds.append((train_idx, valid_idx))\n",
    "\n",
    "\n",
    "\n",
    "    lgb_models={}\n",
    "    for fold in range(n):\n",
    "        train_idx, valid_idx = folds[fold]\n",
    "        X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                             train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "    \n",
    "        \n",
    "        lgb = LGBMClassifier(lambda_l1=0.014630436883633463,\n",
    "      lambda_l2= 0.04929901721885113,\n",
    "      learning_rate= 0.19809719345888235,\n",
    "      max_depth= 43,\n",
    "      min_child_samples= 8938,\n",
    "      min_data_in_leaf= 105,\n",
    "      n_estimators= 818,\n",
    "      num_leaves= 3959)\n",
    "        lgb.fit(X_train, y_train, \n",
    "                eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "                early_stopping_rounds=30,\n",
    "               verbose=100)\n",
    "        lgb_models[fold]=lgb\n",
    "\n",
    "\n",
    "\n",
    "        losses.append(log_loss(y_valid, lgb.predict_proba(X_valid)))\n",
    "    print(sum(losses)/n)\n",
    "    return lgb_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "earned-eugene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's multi_logloss: 0.552073\tvalid_1's multi_logloss: 0.703662\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttraining's multi_logloss: 0.554716\tvalid_1's multi_logloss: 0.759542\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's multi_logloss: 0.52068\tvalid_1's multi_logloss: 0.79728\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's multi_logloss: 0.578966\tvalid_1's multi_logloss: 0.762783\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's multi_logloss: 0.512335\tvalid_1's multi_logloss: 0.737564\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttraining's multi_logloss: 0.516982\tvalid_1's multi_logloss: 0.751308\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's multi_logloss: 0.657353\tvalid_1's multi_logloss: 0.753431\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's multi_logloss: 0.569098\tvalid_1's multi_logloss: 0.760431\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's multi_logloss: 0.512134\tvalid_1's multi_logloss: 0.740199\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's multi_logloss: 0.492876\tvalid_1's multi_logloss: 0.760319\n",
      "0.7526519344981382\n"
     ]
    }
   ],
   "source": [
    "out_models = tr(out_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr(train):\n",
    "    skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=42)\n",
    "    folds=[]\n",
    "    losses=[]\n",
    "    for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "        folds.append((train_idx, valid_idx))\n",
    "\n",
    "\n",
    "\n",
    "    lgb_models={}\n",
    "    for fold in range(n):\n",
    "        train_idx, valid_idx = folds[fold]\n",
    "        X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                             train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "    \n",
    "        \n",
    "        lgb = LGBMClassifier(lambda_l1= 0.04619401127783732,\n",
    "  lambda_l2= 0.023516340914865916,\n",
    "  learning_rate= 0.21142050117535985,\n",
    "  max_depth= 9.817818031569452,\n",
    "  min_child_samples= 1069.5789469296615,\n",
    "  min_data_in_leaf= 159.20038594955219,\n",
    "  n_estimators= 1440.4418354272257,\n",
    "  num_leaves= 3976.0290566566396)\n",
    "        lgb.fit(X_train, y_train, \n",
    "                eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "                early_stopping_rounds=30,\n",
    "               verbose=100)\n",
    "        lgb_models[fold]=lgb\n",
    "\n",
    "\n",
    "\n",
    "        losses.append(log_loss(y_valid, lgb.predict_proba(X_valid)))\n",
    "    print(sum(losses)/n)\n",
    "    return lgb_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bearing-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttraining's multi_logloss: 0.466691\tvalid_1's multi_logloss: 0.727088\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's multi_logloss: 0.450696\tvalid_1's multi_logloss: 0.721642\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's multi_logloss: 0.463337\tvalid_1's multi_logloss: 0.742422\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's multi_logloss: 0.483567\tvalid_1's multi_logloss: 0.716677\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's multi_logloss: 0.452385\tvalid_1's multi_logloss: 0.719332\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's multi_logloss: 0.48659\tvalid_1's multi_logloss: 0.737945\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's multi_logloss: 0.452183\tvalid_1's multi_logloss: 0.716852\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's multi_logloss: 0.452425\tvalid_1's multi_logloss: 0.732606\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's multi_logloss: 0.473286\tvalid_1's multi_logloss: 0.733393\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=105, min_child_samples=8938 will be ignored. Current value: min_data_in_leaf=105\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.014630436883633463, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.014630436883633463\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.04929901721885113, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04929901721885113\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's multi_logloss: 0.439941\tvalid_1's multi_logloss: 0.722784\n",
      "0.7270740868657428\n"
     ]
    }
   ],
   "source": [
    "in_models = tr(in_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-istanbul",
   "metadata": {},
   "source": [
    "# #######모델링, 학습 끝#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "floppy-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_zeros = np.zeros([len(in_test),3])\n",
    "for fold in range(n):\n",
    "    in_zeros += in_models[fold].predict_proba(in_test)/n\n",
    "in_output = pd.DataFrame(in_zeros)\n",
    "in_output = in_output.reindex(index=pd.Index(idx_in_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eight-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_zeros = np.zeros([len(out_test),3])\n",
    "for fold in range(n):\n",
    "    out_zeros += out_models[fold].predict_proba(out_test)/n\n",
    "out_output = pd.DataFrame(out_zeros)\n",
    "out_output = out_output.reindex(index=pd.Index(idx_out_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "prime-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "in2 = np.concatenate((in_zeros,np.array(idx_in_test).reshape(-1,1)),axis=1)\n",
    "out2 = np.concatenate((out_zeros,np.array(idx_out_test).reshape(-1,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "requested-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(np.concatenate((in2,out2),axis=0),columns=[0,1,2,'index'])\n",
    "output['index'] = output['index'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "exclusive-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(output)):\n",
    "    row = output.loc[output['index']==i]\n",
    "    ss.iloc[i,1:] = row.iloc[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "human-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('bayesian_lgbm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
