{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quantitative-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "devoted-biology",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 일 때\n",
      "26457 개를 드랍\n",
      "12.262480299674028\n",
      "11.781654065019929\n",
      "\n",
      "1.0 일 때\n",
      "26457 개를 드랍\n",
      "12.502893417001076\n",
      "11.54124094769288\n",
      "\n",
      "1.5 일 때\n",
      "26457 개를 드랍\n",
      "12.743306534328125\n",
      "11.300827830365831\n",
      "\n",
      "2.0 일 때\n",
      "26457 개를 드랍\n",
      "12.983719651655173\n",
      "11.060414713038783\n",
      "\n",
      "2.5 일 때\n",
      "26457 개를 드랍\n",
      "13.224132768982223\n",
      "10.820001595711734\n",
      "\n",
      "3.0 일 때\n",
      "26457 개를 드랍\n",
      "13.46454588630927\n",
      "10.579588478384686\n",
      "\n",
      "3.5 일 때\n",
      "26457 개를 드랍\n",
      "13.70495900363632\n",
      "10.339175361057636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean +- 1.96 std 95\n",
    "# mean +- 1.6 std 90\n",
    "# mean +- 3.27 std 99\n",
    "# for i,k in enumerate([1.6, 1.96, 3.27]):\n",
    "result = []\n",
    "for i,k in enumerate(range(50,400,50)):\n",
    "    k = 0.01 * k\n",
    "    \n",
    "    d = \"C:\\kaggle_data\\credit_card\"\n",
    "    lst = os.listdir(d)\n",
    "#     print(lst)\n",
    "    train = pd.read_csv(d + '\\\\' +lst[3])\n",
    "    test = pd.read_csv(d + '\\\\' +lst[2])\n",
    "    ss = pd.read_csv(d + '\\\\' +lst[1])\n",
    "    train = train.drop(['index'], axis=1)\n",
    "    train.fillna('NAN', inplace=True) \n",
    "    test = test.drop(['index'], axis=1)\n",
    "    test.fillna('NAN', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Married, Civil marriage\n",
    "    train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "    test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "    train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "    = train['income_per_size'] * 2\n",
    "    test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "    = test['income_per_size'] * 2\n",
    "\n",
    "    def simple_marry(x):\n",
    "        if x == 'Married' or x =='Civil marriage':\n",
    "            return '0'\n",
    "        elif x == 'Separated' or x == 'Widow':\n",
    "            return '1'\n",
    "        else:\n",
    "            return '2'\n",
    "    # 안 덮어쓰기\n",
    "    for df in [train,test]:\n",
    "        df['family_bins'] = df['family_type'].apply(simple_marry)\n",
    "\n",
    "    ### 여기서부터\n",
    "        ### \n",
    "    train['log_income'] = np.log(train['income_total'])\n",
    "    test['log_income'] = np.log(test['income_total'])\n",
    "    \n",
    "        # income_total mean, std 계산\n",
    "    mean_income = train['log_income'].mean()\n",
    "    std_income = train['log_income'].std()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # \n",
    "    idxs = train.loc[(train['log_income']>= mean_income + k*std_income)|\\\n",
    "                    (train['log_income']<= mean_income - k*std_income)].index\n",
    "    train.drop(idxs).reset_index(drop=True,inplace=True)\n",
    "    print(k,'일 때')\n",
    "    print(len(train), \"개를 드랍\")\n",
    "    print(mean_income + k*std_income)\n",
    "    print(mean_income - k*std_income)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     train['careality'] = train['car'] + train['reality']\n",
    "#     train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "#     test['careality'] = test['car'] + test['reality']\n",
    "#     test = test.drop(['car', 'reality'],1)\n",
    "#     object_col = []\n",
    "#     for col in train.columns:\n",
    "#         if train[col].dtype == 'object':\n",
    "#             object_col.append(col)\n",
    "\n",
    "#     enc = OneHotEncoder()\n",
    "#     enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "#     train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "#                  columns=enc.get_feature_names(object_col))\n",
    "#     train.drop(object_col, axis=1, inplace=True)\n",
    "#     train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "#     test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "#                  columns=enc.get_feature_names(object_col))\n",
    "#     test.drop(object_col, axis=1, inplace=True)\n",
    "#     test = pd.concat([test, test_onehot_df], axis=1)\n",
    "    \n",
    "    \n",
    "#     test.head()\n",
    "    \n",
    "#     skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     folds=[]\n",
    "#     losses=[]\n",
    "#     for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "#         folds.append((train_idx, valid_idx))\n",
    "#     random.seed(42)\n",
    "#     lgb_models={}\n",
    "#     for fold in range(5):\n",
    "# #         print(f'===================================={fold+1}============================================')\n",
    "#         train_idx, valid_idx = folds[fold]\n",
    "#         X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "#                                              train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "#         lgb = LGBMClassifier(n_estimators=1000)\n",
    "#         lgb.fit(X_train, y_train, \n",
    "#                 eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "#                 early_stopping_rounds=30, verbose=False)\n",
    "#         lgb_models[fold]=lgb\n",
    "#         losses.append(log_loss(y_valid, lgb.predict_proba(X_valid)))\n",
    "# #         print(f'================================================================================\\n\\n')\n",
    "#     print(len(train), \"개를 드랍\")\n",
    "#     print(i,'번째', 'std*', k, sum(losses)/5)\n",
    "#     result.append([i,k,sum(losses)/5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "nonprofit-equivalent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.388, 0.7339914578558148]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sorted(result, key= lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean +- 1.96 std 95\n",
    "# mean +- 1.6 std 90\n",
    "# mean +- 3.27 std 99\n",
    "# for i,k in enumerate([1.6, 1.96, 3.27]):\n",
    "result = []\n",
    "for i,k in enumerate(range(160,389,1)):\n",
    "    k = 0.001 * k\n",
    "    \n",
    "    d = \"C:\\kaggle_data\\credit_card\"\n",
    "    lst = os.listdir(d)\n",
    "#     print(lst)\n",
    "    train = pd.read_csv(d + '\\\\' +lst[3])\n",
    "    test = pd.read_csv(d + '\\\\' +lst[2])\n",
    "    ss = pd.read_csv(d + '\\\\' +lst[1])\n",
    "    train = train.drop(['index'], axis=1)\n",
    "    train.fillna('NAN', inplace=True) \n",
    "    test = test.drop(['index'], axis=1)\n",
    "    test.fillna('NAN', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Married, Civil marriage\n",
    "    train['income_per_size'] = np.log(train['income_total']/train['family_size'])\n",
    "    test['income_per_size'] = np.log(test['income_total']/test['family_size'])\n",
    "    train.loc[(train['family_type']=='Married')|(train['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "    = train['income_per_size'] * 2\n",
    "    test.loc[(test['family_type']=='Married')|(test['family_type']=='Civil marriage'),'income_per_size']\\\n",
    "    = test['income_per_size'] * 2\n",
    "\n",
    "    def simple_marry(x):\n",
    "        if x == 'Married' or x =='Civil marriage':\n",
    "            return '0'\n",
    "        elif x == 'Separated' or x == 'Widow':\n",
    "            return '1'\n",
    "        else:\n",
    "            return '2'\n",
    "    # 안 덮어쓰기\n",
    "    for df in [train,test]:\n",
    "        df['family_bins'] = df['family_type'].apply(simple_marry)\n",
    "\n",
    "    ### 여기서부터\n",
    "        ### \n",
    "    train['log_income'] = np.log(train['income_total'])\n",
    "    test['log_income'] = np.log(test['income_total'])\n",
    "    \n",
    "        # income_total mean, std 계산\n",
    "    mean_income = train['log_income'].mean()\n",
    "    std_income = train['log_income'].std()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # \n",
    "    idxs = train.loc[(train['log_income']>= mean_income + k*std_income)|\\\n",
    "                    (train['log_income']<= mean_income - k*std_income)].index\n",
    "    train.drop(idxs).reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train['careality'] = train['car'] + train['reality']\n",
    "    train = train.drop(['car', 'reality'],1)\n",
    "\n",
    "    test['careality'] = test['car'] + test['reality']\n",
    "    test = test.drop(['car', 'reality'],1)\n",
    "    object_col = []\n",
    "    for col in train.columns:\n",
    "        if train[col].dtype == 'object':\n",
    "            object_col.append(col)\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "    train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "                 columns=enc.get_feature_names(object_col))\n",
    "    train.drop(object_col, axis=1, inplace=True)\n",
    "    train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "    test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "                 columns=enc.get_feature_names(object_col))\n",
    "    test.drop(object_col, axis=1, inplace=True)\n",
    "    test = pd.concat([test, test_onehot_df], axis=1)\n",
    "    \n",
    "    \n",
    "    test.head()\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    folds=[]\n",
    "    losses=[]\n",
    "    for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "        folds.append((train_idx, valid_idx))\n",
    "    random.seed(42)\n",
    "    lgb_models={}\n",
    "    for fold in range(5):\n",
    "#         print(f'===================================={fold+1}============================================')\n",
    "        train_idx, valid_idx = folds[fold]\n",
    "        X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                             train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "        lgb = LGBMClassifier(n_estimators=1000)\n",
    "        lgb.fit(X_train, y_train, \n",
    "                eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "                early_stopping_rounds=30, verbose=False)\n",
    "        lgb_models[fold]=lgb\n",
    "        losses.append(log_loss(y_valid, lgb.predict_proba(X_valid)))\n",
    "#         print(f'================================================================================\\n\\n')\n",
    "    print(len(train), \"개를 드랍\")\n",
    "    print(i,'번째', 'std*', k, sum(losses)/5)\n",
    "    result.append([i,k,sum(losses)/5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-xerox",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-beginning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-stamp",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-block",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-hungary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-australian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-partner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abstract-latex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['income_total']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-guest",
   "metadata": {},
   "source": [
    "### 여기까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "maritime-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train.drop(object_col, axis=1, inplace=True)\n",
    "train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test.drop(object_col, axis=1, inplace=True)\n",
    "test = pd.concat([test, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-dating",
   "metadata": {},
   "source": [
    "# 모델링, 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hundred-afternoon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spoken-sending",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================1============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.64614\tvalid_1's multi_logloss: 0.751922\n",
      "[200]\ttraining's multi_logloss: 0.559157\tvalid_1's multi_logloss: 0.734178\n",
      "[300]\ttraining's multi_logloss: 0.49697\tvalid_1's multi_logloss: 0.726975\n",
      "[400]\ttraining's multi_logloss: 0.443208\tvalid_1's multi_logloss: 0.725848\n",
      "Early stopping, best iteration is:\n",
      "[373]\ttraining's multi_logloss: 0.457287\tvalid_1's multi_logloss: 0.724812\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================2============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.64376\tvalid_1's multi_logloss: 0.761664\n",
      "[200]\ttraining's multi_logloss: 0.55681\tvalid_1's multi_logloss: 0.74422\n",
      "Early stopping, best iteration is:\n",
      "[238]\ttraining's multi_logloss: 0.530272\tvalid_1's multi_logloss: 0.74203\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================3============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.645449\tvalid_1's multi_logloss: 0.757937\n",
      "[200]\ttraining's multi_logloss: 0.555083\tvalid_1's multi_logloss: 0.743417\n",
      "[300]\ttraining's multi_logloss: 0.488788\tvalid_1's multi_logloss: 0.741023\n",
      "Early stopping, best iteration is:\n",
      "[346]\ttraining's multi_logloss: 0.463785\tvalid_1's multi_logloss: 0.740401\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================4============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.643362\tvalid_1's multi_logloss: 0.750922\n",
      "[200]\ttraining's multi_logloss: 0.556491\tvalid_1's multi_logloss: 0.735643\n",
      "[300]\ttraining's multi_logloss: 0.491385\tvalid_1's multi_logloss: 0.729211\n",
      "Early stopping, best iteration is:\n",
      "[337]\ttraining's multi_logloss: 0.472174\tvalid_1's multi_logloss: 0.72821\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================5============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.64778\tvalid_1's multi_logloss: 0.751894\n",
      "[200]\ttraining's multi_logloss: 0.558684\tvalid_1's multi_logloss: 0.738642\n",
      "[300]\ttraining's multi_logloss: 0.493246\tvalid_1's multi_logloss: 0.734328\n",
      "Early stopping, best iteration is:\n",
      "[330]\ttraining's multi_logloss: 0.475754\tvalid_1's multi_logloss: 0.733426\n",
      "================================================================================\n",
      "\n",
      "\n",
      "0.7337758026841164\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "folds=[]\n",
    "losses=[]\n",
    "for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "random.seed(42)\n",
    "lgb_models={}\n",
    "for fold in range(5):\n",
    "    print(f'===================================={fold+1}============================================')\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                         train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "    lgb = LGBMClassifier(n_estimators=1000)\n",
    "    lgb.fit(X_train, y_train, \n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "            early_stopping_rounds=30,\n",
    "           verbose=100)\n",
    "    lgb_models[fold]=lgb\n",
    "    losses.append(log_loss(y_valid, lgb.predict_proba(X_valid)))\n",
    "    print(f'================================================================================\\n\\n')\n",
    "print(sum(losses)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현 baseline의 점수\n",
    "0.7337758026841164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "twelve-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.iloc[:,1:]=0\n",
    "for fold in range(5):\n",
    "    ss.iloc[:,1:] += lgb_models[fold].predict_proba(test)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sustainable-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.to_csv('5_marriage_type.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
